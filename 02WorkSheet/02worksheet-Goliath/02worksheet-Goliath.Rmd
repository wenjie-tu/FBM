---
title: "Worksheet 2"
author: 'Goliath : Wenje Tu, Lea BÃ¼hrer, Jerome Sepin, ...'
date: "Spring Semester 2022"
output: pdf_document
# bibliography: biblio.bib
# nocite: '@*'
subtitle: Foundations of Bayesian Methodology
papersize: a4
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Exercise 3 (Conjugate Bayes: analytical derivation)

Assumptions: 
$$
\begin{aligned}
y_1,\cdots,y_n\mid m  & \overset{i.i.d}{\sim} \mathcal{N}(m,\kappa^{-1}) \\
m & \: \sim \:\mathcal{N}(\mu,\lambda^{-1})
\end{aligned}
$$

Likelihood function:
$$
\begin{aligned}
f(y_1,\cdots,y_n\mid m)
&= \prod_{i=1}^{n} f(y_i\mid m) \\
&= \prod_{i=1}^{n} \left(\sqrt{\frac{\kappa}{2\pi}}\exp\left(-\frac{\kappa}{2}(y_i-m)^2 \right) \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\exp\left(-\frac{\kappa}{2}\sum_{i=1}^{n}(y_i-m)^2 \right)
\end{aligned}
$$

Prior density distribution of $m$: 
$$
f(m)=\sqrt{\frac{\lambda}{2\pi}}\exp\left(-\frac{\lambda}{2}(m-\mu)^2 \right)
$$

Multiply likelihood function by prior: 
$$
\begin{aligned}
&\quad\quad f(y_1,\cdots,y_n\mid m)f(m) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\exp\left(-\frac{\kappa}{2}\sum_{i=1}^{n}(y_i-m)^2 \right)
\cdot \sqrt{\frac{\lambda}{2\pi}}\exp\left(-\frac{\lambda}{2}(m-\mu)^2 \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{\kappa}{2}\sum_{i=1}^{n}(y_i-m)^2-\frac{\lambda}{2}(m-\mu)^2 \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{\kappa}{2}\sum_{i=1}^{n}(y_i^2-2y_im+m^2)-\frac{\lambda}{2}(m^2-2m\mu+\mu^2) \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{\kappa}{2}\left(\sum_{i=1}^{n}y_i^2-2n\bar{y}m+n m^2\right)-\frac{\lambda}{2}(m^2-2m\mu+\mu^2) \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{1}{2}\left((n\kappa+\lambda)m^2-2m(\kappa n\bar{y}+\lambda\mu)+\kappa\sum_{i=1}^{n}y_i^2+\lambda\mu^2 \right) \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{n\kappa+\lambda}{2}\left(m^2-2m\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}+\frac{\kappa\sum_{i=1}^{n}y_i^2+\lambda\mu^2}{n\kappa+\lambda} \right) \right) \\
&= \left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(-\frac{n\kappa+\lambda}{2}\left(\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2-\left(\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}\right)^2+\frac{\kappa\sum_{i=1}^{n}y_i^2+\lambda\mu^2}{n\kappa+\lambda} \right) \right) \\
&= \underbrace{
\left(\frac{\kappa}{2\pi} \right)^{\frac{n}{2}}\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}
\exp\left(\frac{\kappa\sum_{i=1}^{n}y_i^2+\lambda\mu^2}{n\kappa+\lambda}-\left(\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}\right)^2\right)
}_\text{constant}
\exp\left(-\frac{n\kappa+\lambda}{2}\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2  \right) \\
\end{aligned}
$$

Using Bayes formula, we can write posterior as a function of prior and likelihood:
$$
\begin{aligned}
f(m\mid y_1,\cdots, y_n)
&= \frac{f(y_1,\cdots,y_n\mid m)f(m)}{f(y_1,\cdots,y_n)} \\
&= \frac{f(y_1,\cdots,y_n\mid m)f(m)}{\int_{-\infty}^{\infty}f(y_1,\cdots,y_n\mid m)f(m)\textrm{d}m} \\
&= \frac{\exp\left(-\frac{n\kappa+\lambda}{2}\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2  \right)}
{\int_{-\infty}^{\infty}\exp\left(-\frac{n\kappa+\lambda}{2}\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2  \right)}  \\
&= \sqrt{\frac{n\kappa+\lambda}{2\pi}}\exp\left(-\frac{n\kappa+\lambda}{2}\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2  \right) 
\end{aligned}
$$

$$
m\mid y_1,\cdots,y_n\sim \mathcal{N}
\left(\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}, \frac{1}{n\kappa+\lambda}\right)
$$

Note:
$$
\int_{-\infty}^{\infty}e^{-a(x+b)^2}\textrm{d}x=\sqrt{\frac{\pi}{\alpha}}
$$
Source: [Gaussian Integral](https://en.wikipedia.org/wiki/Gaussian_integral)

$~$

### Exercise 4 (Conjugate Bayesian analysis in practice)

Assumptions: 

$$
\begin{aligned}
y_1,\cdots,y_n\mid m  & \overset{i.i.d}{\sim} \mathcal{N}(m,\kappa^{-1}) \\
m & \: \sim \:\mathcal{N}(\mu,\lambda^{-1}) 
\end{aligned}
$$

$$
\kappa=\frac{1}{900},\quad \mu=161, \quad \lambda=\frac{1}{70}
$$

#### 4(a)

Summary statistics:

```{r}
height <- c(166, 168, 168, 177, 160, 170, 172, 159, 175, 164, 175, 167, 164)
summary(height)
```

```{r, out.width="50%", fig.align='center', fig.cap="Histogram of the height measurements\\label{fig:hist}"}
hist(height, breaks=length(height), freq=FALSE)
lines(density(height), col="red")
```

```{r, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%", fig.cap="Q-Q Plot of the Measurements\\label{fig:qqhueght}"}
library(car)
qqPlot(height, main="Normal Q-Q Plot", ylab="Sample", xlab="Norm Quantiles")

# or alternatively without library(car)
qqnorm(height)
qqline(height)
```

In the Figure \ref{fig:qqhueght} (i.e. Q-Q Plot), it can be seen that all sample values lie within the area where normal distribution of the data can be assumed. Due to the fact that we only have 13 observations the 95%-CI has been calculated assuming a t-distribution. 


```{r}
## Sample size
sample.n <- length(height); sample.n

## Sample median
sample.med <- median(height); sample.med

## Sample meam
sample.mean <- mean(height); sample.mean

## Sample standard deviation
sample.sd <- sd(height); sample.sd

## Sample standard error
sample.se <- sample.sd / sqrt(sample.n); sample.se
```

```{r}
## Significance level
alpha <- 0.05

## Degrees of freedom
df <- sample.n - 1

## t-score
t.score <- qt(p=alpha/2, df=df, lower.tail=FALSE); t.score
```

```{r}
## Contract the 95% confidence interval
lower.bound <- sample.mean - t.score * sample.se
upper.bound <- sample.mean + t.score * sample.se
print(c(lower.bound, upper.bound))
```

\begin{table}[!htbp]
    \centering
    \caption{Summary statistics of the sample distribution}
    \begin{tabular}{lcccc}
    \hline
                      & Mean       & Standard deviation & Median     & $95\%$ CI              \\
    \hline
    $y_1,\cdots,y_n$  & $168.0769$ & $5.6341$           & $168.0000$ & $(164.6722, 171.4816)$ \\
    \hline
    \end{tabular}
    \label{tab:summary}
\end{table}

**Interpretation**: For repeated random samples from a normal distribution with unknown but fixed mean, the 95% confidence interval $(164.6722, 171.4816)$ will cover the true unknown mean in 95% of all cases.

<!-- Based on our data, the interval $(164.6722, 171.4816)$ gives us a range of parameters which are, based on this data, the best 95% parameters for the true unknown parameter of interest (mean). We do not know whether this Confidence Interval covers the parameter of interest but it is constructed in a way that if repeated sampling, estimation and construction of the confidence interval in 95% of all cases the confidence interval covers the true unknown parameter of interest. -->

#### 4(b)

We first plot the prior distribution of $m$:

```{r, out.width="50%", fig.align='center', fig.cap="Prior Distribution\\label{fig:prior}"}
## Plot of the Prior Distribution: 
m <- seq(101, 221)
plot(m, dnorm(m, mean=161, sd=sqrt(70)), type="l", ylab="density", col="red", 
     ylim=c(0,0.07), main="Prior distribution", lwd=2)
legend("topright", legend=c("prior: N(161,70)"), lwd=2, 
       lty=1, col=c("red"), cex=.8)
```

<!-- We want to predict the probability that m > 200. We therefore first compute the expectation, standard deviation, median and equi-tailed 95% interval of the prior distribution.  -->

The expectation and the standard deviation can be explicitly obtained from the given information. Hence, we only need to compute the median and equi-tailed 95% interval. We use \texttt{qnorm()} function in \texttt{R} to serve this purpose. Regarding the estimation of $P[m>200]$, we simply use \texttt{pnorm()} in \texttt{R} to obtain the corresponding probability.

$$
P[m>200]=1-P[m\leq200]
$$

<!-- The expected value as well as the standard deviation do not need to be calculated due to the fact that they are given. Using a normal distribution we know $\mu$ and $\sigma^2$ which are the estimates for the expected value and the variance.  -->

```{r}
## 2.5%, 50%(i.e. median), 97.5% quantiles of N(161,70)
qnorm(c(0.025, 0.5, 0.975), mean=161, sd=sqrt(70))

## P[m>200]
pnorm(200, mean=161, sd=sqrt(70), lower.tail=FALSE)
```

Summary statistics: see Table \ref{tab:summary}

#### 4(c)

$$
\begin{aligned}
y_1,\cdots,y_n\mid m  & \overset{i.i.d}{\sim} \mathcal{N}(m,\kappa^{-1}) \\
m & \: \sim \:\mathcal{N}(\mu,\lambda^{-1}) \\
m\mid y_1,\cdots,y_n & \:\sim\: \mathcal{N}
\left(\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}, (n\kappa+\lambda)^{-1} \right)
\end{aligned}
$$

```{r}
## Posterior mean
((1/900)*2185+(1/70)*161 ) / (13*(1/900)+(1/70) )

## Posterior variance
(13*(1/900)+(1/70))^(-1)
```

$$
\begin{aligned}
y_1,\cdots,y_n &\sim \mathcal{N}(m, 900) \\
m &\sim \mathcal{N}(161,70) \\
m\mid y_1,\cdots,y_n &\sim \mathcal{N}(164.558, 34.80663)
\end{aligned}
$$

```{r, out.width="50%", fig.align='center', fig.cap="Prior vs. Posterior\\label{fig:prior-post}"}
## Visualization of prior and posterior
m <- seq(101, 221)
plot(m, dnorm(m, mean=161, sd=sqrt(70)), type="l", ylab="density", col="red", 
     ylim=c(0,0.08), main="Prior vs Posterior", lwd=2)
lines(m, dnorm(m, mean=164.558, sd=sqrt(34.80663)), col="blue", lwd=2)
legend("topright", legend=c("prior: N(161,70)", "posterior: N(164.56, 34.81)"), 
       lty=1, col=c("red", "blue"), cex=.8, lwd=2)
```

```{r}
qnorm(c(0.025, 0.5, 0.975), mean=164.558, sd=sqrt(34.80663))
```

\begin{table}[!htbp]
    \centering
    \caption{Summary statistics of the sample, prior and posterior distributions}
    \begin{tabular}{lcccc}
    \hline
                           & Mean       & Standard deviation & Median     & Equi-tailed $95\%$ CI/CrI \\
    \hline
    $y_1,\cdots,y_n$       & $168.0769$ & $5.6341$           & $168.0000$ & $(164.6722, 171.4816)$ \\
    $m$                    & $161.0000$ & $8.3666$           & $161.0000$  & $(144.6018,177.3982)$           \\
    $m\mid y_i,\cdots,y_n$ & $164.5580$  & $5.8997$          & $164.5580$  & $(152.9948,176.1212)$           \\
    \hline
    \end{tabular}
    \label{tab:summary}
\end{table}

**Interpretation:** the posterior probability of the \texttt{Height} $m$ lies between 152.9948 and 176.1212 with a probability of 95%, given a $\mathcal{N}(161,70)$ prior is assumed.

#### 4(d)

$$
P[m>200\mid y_1, ..., y_n]
$$

```{r}
## P[m>200|y1,...,yn]
pnorm(200, mean=164.558, sd=sqrt(34.80663), lower.tail=FALSE) 
# or
1 - pnorm(200, mean=164.558, sd=sqrt(34.80663))
```

The posterior probability that an adult Swiss female has a height larger than 200 is $9.426\times10^{-10}$.

<!-- The probability of having a value for m that's larger than 200, given the above defined posterior and prior distribution is $9.426 e-10$. -->

#### 4(e)

$$
\begin{aligned}
\textrm{Prior} 
&\to \textrm{Posterior} \\
\mathcal{N}(161,70) 
&\to \mathcal{N}(164.558, 34.80663) \\
P[m>200] 
&\to P[m>200\mid y_1,\cdots,y_n] \\
1.570393\times 10^{-6} 
&\to 9.425552\times 10^{-10}
\end{aligned}
$$

From prior to posterior, we see an increase in the mean of $m$ from 161 to 164.558 and a decrease in the variance of $m$ from 70 to 34.80663. Figure \ref{fig:prior-post-likelihood} displays a huge overlap between the prior distribution and the likelihood density, it is not surprising that the posterior mean lies somewhere between the prior mean and the sample mean and that the posterior variance lies somewhere between the prior variance and sample variance. Since both prior and likelihood mostly agree, we see a more concentrated posterior distribution with light tails. Thus, the probability of observing a Swiss female with a height greater than 200 also decreases. 

<!-- It is noticeable that the variance in particular has decreased from prior to posterior distribution. Thus the dispersion is smaller. This leads to the distribution becoming narrower and the probabilities around the mean increase. Thus, the probability of observing a value greater than 200 also decreases.  -->

```{r, out.width="50%", fig.align='center', fig.cap="Prior vs. Likelihood vs. Posterior \\label{fig:prior-post-likelihood}"}
m <- seq(101, 221)
plot(m, dnorm(m, mean=161, sd=sqrt(70)), type="l", ylab="density", col="red", 
     ylim=c(0,0.08), main="Prior vs Likelihood vs Posterior", lwd=2)
lines(m, dnorm(m, mean=164.558, sd=sqrt(34.80663)), col="blue", lwd=2)
lines(density(height), col="orange", lwd=2)
legend("topright", lty=1, col=c("red", "blue", "orange"), cex=.8, lwd=2, 
       legend=c("prior: N(161,70)", "posterior: N(164.56, 34.81)", "likelihood density"))
```

$~$

### Exercise 5 (Bayesian learning)

Prior distribution:
$$
p\sim \textrm{Beta}(\alpha,\beta)
$$

Posterior distribution:
$$
p\mid y_1,\cdots,y_n \sim \textrm{Beta}(\alpha+n\bar{y},\beta+n-n\bar{y})
$$

\begin{table}[!htbp]
    \centering
    \caption{Evidence $P(\textrm{response rate}>0.4)$ for each stage under different priors}
    \begin{tabular}{llllll}
    \hline
    Prior             & Stage    & \#   & Responders        & Posterior               & Evidence                        \\
    $B(\alpha,\beta)$ &          & $n$  & $x\:(\%)$         & $B(\alpha+x,\beta+n-x)$ & $P(\textrm{response rate}>0.4)$ \\
    \hline
                      & No data  & $0$  & $0\:(0\%)$        & $B(0.5,0.5)$            & $0.5640942$                     \\
    $B(0.5,0.5)$      & Interim  & $12$ & $3\:(25\%)$       & $B(3.5,9.5)$            & $0.1437649$                     \\
                      & Final    & $64$ & $14\:(21.875\%)$  & $B(14.5,50.5)$          & $0.001075757$                   \\
                      &          &      &                   &                         &                                 \\
                      & No data  & $0$  & $0\:(0\%)$        & $B(8,24)$               & $0.03298768$                    \\
    $B(8,24)$         & Interim  & $12$ & $3\:(25\%)$       & $B(11, 33)$             & $0.01621346$                    \\
                      & Final    & $64$ & $14\:(21.875\%)$  & $B(22,74)$              & $0.0001727695$                  \\
    \hline
    \end{tabular}
    \label{tab:response}
\end{table}

\begin{table}[!htbp]
    \centering
    \caption{Summary statistics of posterior distributions}
    \begin{tabular}{lllccc}
    \hline
                 &          & Posterior         & Mean                          & Median        & Equi-tailed $95\%$ CrI \\
    Prior        & Stage    & $B(\alpha,\beta)$ & $\frac{\alpha}{\alpha+\beta}$ & 50\% quantile &                        \\
    \hline
                 & No data  & $B(0.5,0.5)$      & $0.5000$   & $0.5000$ & $(0.0015, 0.9985)$  \\
    $B(0.5,0.5)$ & Interim  & $B(3.5,9.5)$      & $0.2692$   & $0.2571$ & $(0.0759, 0.5292)$  \\
                 & Final    & $B(14.5,50.5)$    & $0.2231$   & $0.2202$ & $(0.1313, 0.3310)$  \\
                 &          &                   &            &          &                     \\
                 & No data  & $B(8,24)$         & $0.2500$   & $0.2447$ & $(0.1186, 0.4110)$  \\
    $B(8,24)$    & Interim  & $B(11,33)$        & $0.2500$   & $0.2461$ & $(0.1352, 0.3863)$  \\
                 & Final    & $B(22,74)$        & $0.2292$   & $0.2273$ & $(0.1512, 0.3178)$  \\
    \hline
    \end{tabular}
    \label{tab:post-summary}
\end{table}

#### 5(a)

$\alpha=\beta=0.5,\quad \textrm{Beta}(0.5,0.5)$

```{r, figures-side, fig.show="hold", out.width="50%", fig.cap="Prior B(0.5,0.5)"}
p <- seq(1e-3,1, length=200)

plot(p, dbeta(p, 0.5, 0.5), type="l", ylab="density", col="red", 
     ylim=c(0, 10), main="Intermediate study", lwd=2)
lines(p, dbeta(p, 3.5, 9.5), ylab="density", col="blue", lwd=2)

legend("topright", legend=c("prior: B(0.5,0.5)", "posterior: B(3.5,9.5)"), 
       col=c("red", "blue"), lty=1, cex=.8, lwd=2)

plot(p, dbeta(p, 0.5, 0.5), type="l", ylab="density", col="red", 
     ylim=c(0, 10), main="Final study", lwd=2)
lines(p, dbeta(p, 14.5, 50.5), ylab="density", col="blue", lwd=2)

legend("topright", legend=c("prior: B(0.5,0.5)", "posterior: B(14.5,50.5)"), 
       col=c("red", "blue"), lty=1, cex=.8, lwd=2)
```

$$
P[\textrm{response rate}>0.4]=P[p>0.4]=1-P[p\leq 0.4]
$$

```{r}
# Before seeing any data (i.e. only prior belief)
pbeta(0.4, 0.5, 0.5, lower.tail=FALSE)

# Intermediate study
pbeta(0.4, 3.5, 9.5, lower.tail=FALSE)

# Final study
pbeta(0.4, 14.5, 50.5, lower.tail=FALSE)
```

```{r}
# Before observing data
## 2.5%, 50%, 97.5% quantiles of Beta(0.5, 0.5)
qbeta(c(0.025, 0.5, 0.975), 0.5, 0.5)

# Intermediate study
## 2.5%, 50%, 97.5% quantiles of Beta(3.5, 9.5)
qbeta(c(0.025, 0.5, 0.975), 3.5, 9.5)

# Final study
## 2.5%, 50%, 97.5% quantiles of Beta(14.5, 50.5)
qbeta(c(0.025, 0.5, 0.975), 14.5, 50.5)
```

#### 5(b)

$\alpha=8,\beta=24,\quad \textrm{Beta}(8,24)$

$$
P[\textrm{response rate}>0.4]=P[p>0.4]=1-P[p\leq 0.4]
$$

```{r, fig.show="hold", out.width="50%", fig.cap="Prior B(8,24)"}
p <- seq(1e-3,1, length=200)

plot(p, dbeta(p, 8, 24), type="l", ylab="density", col="red", 
     ylim=c(0, 10), main="Intermediate study", lwd=2)
lines(p, dbeta(p, 11, 33), ylab="density", col="blue", lwd=2)

legend("topright", legend=c("prior: B(8,24)", "posterior: B(11,33)"), 
       col=c("red", "blue"), lty=1, cex=.8, lwd=2)

plot(p, dbeta(p, 8, 24), type="l", ylab="density", col="red", 
     ylim=c(0, 10), main="Final study", lwd=2)
lines(p, dbeta(p, 22, 74), ylab="density", col="blue", lwd=2)

legend("topright", legend=c("prior: B(8,24)", "posterior: B(22,74)"), 
       col=c("red", "blue"), lty=1, cex=.8, lwd=2)
```

```{r}
# Before seeing any data
pbeta(0.4, 8, 24, lower.tail=FALSE)

# Intermediate study
pbeta(0.4, 11, 33, lower.tail=FALSE)

# Final study
pbeta(0.4, 22, 74, lower.tail=FALSE)
```

```{r}
# Before observing data
## 2.5%, 50%, 97.5% quantiles of Beta(8, 24)
qbeta(c(0.025, 0.5, 0.975), 8, 24)

# Intermediate study
## 2.5%, 50%, 97.5% quantiles of Beta(11, 33)
qbeta(c(0.025, 0.5, 0.975), 11, 33)

# Final study
## 2.5%, 50%, 97.5% quantiles of Beta(22, 74)
qbeta(c(0.025, 0.5, 0.975), 22, 74)
```
