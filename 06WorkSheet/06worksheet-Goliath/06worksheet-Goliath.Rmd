---
title: "Worksheet 6"
author: 
  - Wenje Tu
  - Lea Bührer
  - Jerome Sepin
  - Zhixuan Li
  - Elia-Leonid Mastropietro
  - Jonas Raphael Füglistaler
date: "Spring Semester 2022"
output: pdf_document
# bibliography: biblio.bib
# nocite: '@*'
subtitle: Foundations of Bayesian Methodology
papersize: a4
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang="us_en")
rm(list=ls())
```

```{r libraries, warning=FALSE, message=FALSE, echo=F}
library(ggplot2)
library(bayesmeta)
library(rjags)
library(coda)
library(car)
library(plotrix)
```

# Exercise 3 (Bayesian meta-analysis with `bayesmeta`)

The aim of the exercise is to compute a Bayesian meta-analysis of log(OR) of treatment and placebo based on eight historical studies. The historical data of the responders in placebo and treatment can be found in Table 1. The data comes from the Baeten et al. study [1] and has been used for the prior elicitation. 

## Log odds ratio and its standard error

From the chapter *Comparison of proportions* on pages 137 and 138 in the book "Likelihood and Bayesian Inference" [2] we know that the log odds ratio y equals:

$$
y=\log(\text{OR})=\log\bigg(\frac{x_\text{P}}{n_\text{P}-x_\text{P}}\bigg)-
\log\bigg(\frac{x_\text{T}}{n_\text{T}-x_\text{T}}\bigg)
$$
And that the standard error equals
$$
\sigma=\text{SE}\left(\log(\text{OR})\right)=
\sqrt{\frac{1}{x_\text{P}} +\frac{1}{n_\text{P}-x_\text{P}} + 
\frac{1}{x_\text{T}} +\frac{1}{n_\text{T}-x_\text{T}}
}
$$

```{r data}
pl_total <- c(107, 44, 51, 39, 139, 20, 78, 35)
pl_case <- c(23, 12, 19, 9, 39, 6, 9, 10)
tr_total <- c(208, 38, 150, 45, 138, 20, 201, 34)
tr_case <- c(120, 18, 107, 26, 82, 16, 126, 23)
log_or <- log(pl_case/(pl_total-pl_case)) - log(tr_case/(tr_total-tr_case))
log_or_se <- sqrt(1/pl_case + 1/(pl_total-pl_case) + 1/tr_case + 1/(tr_total-tr_case))
labels <- 1:length(pl_total)
```

```{r data-table, warning=F}
dat <- data.frame(labels, tr_case, tr_total, pl_case, pl_total, log_or, log_or_se)
knitr::kable(dat, align="c", digits=3, caption="Historical data for meta-analysis")
```


<!-- From Table 1 the overall estimate for the OR and log(OR) can easily be calculated, when the studies are all independent and are identical realizations of the same underlying process (i.i.d). In total 834 patients have been treated, of which 518 have responded to the treatment. In the placebo group 127 from 513 patients have responded. See table below. -->


```{r OR, echo=F, include=F, eval=F}
## There might be some mistakes here
b <- sum(tr_case)
d <- sum(tr_total) - sum(tr_case)
a <- sum(pl_case)
c <- sum(pl_total) - sum(pl_case)
tabledat <- matrix(c(a,b,c,d), nrow = 2)

colnames(tabledat) <- c("Non-responder", "Responder ")
rownames(tabledat) <- c("Treatment", "Placebo")
knitr::kable(tabledat)
```

```{r, include=F, eval=F}
tr_response1 <- sum(tr_case)
tr_response0 <- sum(tr_total) - sum(tr_case)
pl_response1 <- sum(pl_case)
pl_response0 <- sum(pl_total) - sum(pl_case)

d.table <- data.frame(matrix(c(tr_response1, tr_response0, 
                               pl_response1, pl_response0), nrow=2, byrow=TRUE))

colnames(d.table) <- c("Responder", "Non-responder")
rownames(d.table) <- c("Treatment", "Placebo")
knitr::kable(d.table, caption="Two-by-two contingency table")
```

<!-- $$ -->
<!-- \text{Odds Ratio}=\frac{\text{Odds in treatment group}}{\text{Odds in placebo group}} -->
<!-- $$ -->

```{r, include=F, eval=F}
## Compute the odds ratio
OR <- (tr_response1 / tr_response0) / (pl_response1 / pl_response0); OR

## Compute the log odds ratio
log_OR <- log(OR); log_OR

## Standard Error of the OR
```


```{r, include=F, eval=F}
## There might be some mistakes here
OR <- (a*d)/(b*c)
round(OR, 4)

log_OR <- log(OR)
round(log_OR, 4)

# Standard Error of the OR:
se_OR <- (1/a+1/b+1/c+1/d)

# Confidence Intervals
CI_OR <- OR + c(-1, 1) * qt(0.975, 7) * se_OR
CI_OR
log_CI_OR <- log(CI_OR)
log_CI_OR
```

```{r, fig.cap="QQ - Plot of the log(OR) from the eight historical studies.\\label{fig:qqPlot}", fig.show='hold', out.width='75%', fig.align='center', echo=F, include=F, eval=F}

qqPlot(dat$log_or, xlab = "Theoretical Quantiles", ylab="Sample Quantiles")
```



<!-- The OR is equal to 0.2007 with a 95% Wald Confidence interval going 0.164 up to 0.238. The confidence interval has been computed using the estimate of the OR and the standard error of the OR. Due to the fact that the eight values for the log(OR)  do not strictly follow a normal distribution the quantiles of a $t$-distribution with $df=7$ have been used to approximate the confidence interval, see Figure \ref{fig:qqPlot}. -->

<!-- The log(OR) equals -1.6059 for the disaggregeated data having a 95% Wald confidence interval going from -1.808 up to 1.438.  -->

<!-- But the strong assumptions of i.i.d and homogeneity between all studies can't be justified in this case.  -->

<!-- So a full Bayesian meta-analysis expressed by the Bayesian normal-normal hierarchical model (NNHM) with three levels of hierarchy is considered in the next step. The three levels of hierarchies are the likelihood, the random effects model and the priors.  -->

## Full bayesian meta-analysis - theory

We consider the full Bayesian meta-analysis expressed by the Bayesian normal-normal hierarchical model (NNHM) with three levels of hierarchy.

Likelihood:
$$
y_i\sim\text{N}(\theta_i,\sigma_i^2)
$$
for $i=1, \cdots, k$

Random effects:
$$
\theta_i\sim\text{N}(\mu,\tau^2)
$$
Priors:
$$
\begin{aligned}
\mu & \sim \text{N}(\nu,\gamma^2) \\
\tau & \sim \lvert \text{N}(0, A^2) \rvert=\text{HN}(A)
\end{aligned}
$$
where $\nu=0, \gamma=4, A=0.5$


$\sigma_i$ represents the within-study standard deviation of the $i$-th study. This value is assumed to be fixed (known). The heterogeneity of random effects is denoted by $\tau$ which represents the between-study standard deviation.

The Bayes Theorem for the Bayesian NNHM reads:

$$
f(\mu,\tau,\boldsymbol\theta\mid (y_1,\sigma_1),\cdots,(y_k, \sigma_k)) = f((y_1,\sigma_1),\cdots,(y_k, \sigma_k)\mid\boldsymbol\theta)\cdot
f(\boldsymbol\theta\mid\mu, \tau) \cdot f(\mu) \cdot f(\tau)\cdot C^{-1}
$$

where $\boldsymbol\theta = \{\theta_1, \cdots\theta_k\}$ and $C = f((y_1, \sigma_1),\cdots,(y_k,\sigma_k))$ is a normalizing constant obtained by integrating out parameters $\mu, \tau, \boldsymbol\theta$ in the numerator of the above printed equation. 


The approximation of the log-posterior is then:

$$
\log(\text{Posterior})\approx \log(\text{Likelihood})+\log(\text{Random-effects model})+
\log(\text{Prior})
$$

Further details can be found in the script [3].



In the following a NNHM is applied to the data obtained in Table 1. The model is defined in the formulas for the Likelihood, Random effect and Prior above. We fit the model numerically with the help of the function `bayesmeta` from the package `bayesmeta` [4]. This function allows to derive the posterior distribution of the two parameters in a random-effects meta-analysis and provides functions to evaluate joint and marginal posterior probability distributions and more. 

## Full bayesian meta-analysis - R implementation and visualization

```{r}
full.bayes <- bayesmeta(y = dat[, "log_or"], 
                        sigma = dat[, "log_or_se"],  
                        labels = dat[, "labels"], 
                        mu.prior.mean = 0, mu.prior.sd = 4,
                        tau.prior = function(t){dhalfnormal(t, scale = 0.5)}, 
                        interval.type = "central",
                        predict = T)
```

The summary of the function returns a matrix listing some summary statistics, namely marginal posterior mode, median, mean, standard deviation and a (shortest) 95% credible intervals, of the marginal posterior distributions of $\tau$ and $\mu$, and of the posterior predictive distribution of $\theta$. See Table 2 for the condensed summary statistics. 

```{r}
summary(full.bayes)
```

```{r, echo=F}
knitr::kable(t(full.bayes$summary), align="c", digits=4, 
             caption="Summary statistics for parameters (bayesmeta)")
```


```{r forest-plot, fig.show='hold', out.width='75%', fig.align='center', fig.cap="Forest Plot provided by `bayesmeta` with the data from Table 1.\\label{fig:forestplot}", echo=F}
forestplot(full.bayes)
```

Figure \ref{fig:forestplot} illustrates the forest plot of the model printed with the function `forestplot`. It shows eight estimates ($y_i$) with their 95% confidence intervals (the eight black horizontal lines on the right) along with the combined estimate (the diamond at the bottom, centered at the posterior median and spanning the 95% interval) and a predictive interval (the rectangle at the bottom, spanning the 95% prediction interval). The prediction interval is always longer than the posterior interval for the effect $\mu$, and it indicates the expected range for a "new" estimate $\theta_\text{new}$. The shrinkage estimates are shown along with the original data as grey horizontal lines. As the name indicates, these are usually shrunk towards the overall mean to some degree.



```{r plots, fig.show='hold', fig.cap="Plots provided by \texttt{bayesmeta} with the data from Table 1.\\label{fig:bayesplots}", echo=F, out.width='50%', results='asis'}
plot(full.bayes, prior=T)
```

Figure \ref{fig:bayesplots} shows the four plots given by the `plot` function of `bayesmeta`. 

* The first plot is another simple forest plot, showing the 8 estimates along with the combined estimate (diamond) and prediction interval (bar).
* The second plot illustrates the joint posterior density of both parameters $\mu$ and $\tau$; a darker shading indicates higher posterior density values. The red contour lines show (approximate) 90%, 95% and 99% confidence regions for the joint distribution. The solid blue line traces the conditional posterior expectation value $\mathbb{E}[\mu\mid\tau,y, \sigma]$, and the dashed lines enclose the corresponding 95% interval as a function of $\tau$. The green lines indicate marginal posterior median and 95% intervals for both parameters.
* The third and fourth plot show the marginal density functions of $\mu$ and $\tau$, respectively. The posterior median and (highest posterior density) 95% interval are also indicated by a vertical line and a darker shading. The dashed line shows the prior density in comparison.



```{r, fig.show='hold', out.width='50%', fig.cap="Marginal posterior distributions for all parameters in the model provided by `bayesmeta` with the data from Table 1. \\label{fig:marginalposteriors}"}
## Compare resulting marginal densities;

## The effect parameter (mu):
mu <- seq(-2.5, -.5, le=200)
plot(mu, full.bayes$dposterior(mu=mu), type="l", lty="solid",
     xlab=expression("effect "*mu),
     ylab="",
     main="",
     ylim=c(0,2.5))


## The heterogeneity parameter (tau):
tau <- seq(0, 1, le=200)
plot(tau, full.bayes$dposterior(tau=tau), type="l", lty="solid",
     xlab=expression("heterogeneity "*tau),
     ylab="",
     main="",
     ylim=c(0,2.5))


## Show the individual effects' posterior distributions:
theta <- seq(-3, -0.5, le=300)
plot(range(theta), c(0,2.5), type="n", xlab=expression(theta[i]), ylab="")
for (i in 1:full.bayes$k) {
  ## Draw effect's posterior distribution:
  lines(theta, full.bayes$dposterior(theta=theta, indiv=i), 
        col="darkblue", lty="solid")
}
abline(h=0)

theta <- seq(-2.5, -0.5, le=200)

plot(theta, full.bayes$dposterior(theta=theta, predict=T), type="l", lty="dashed",
     xlab=expression("effect "*theta[new]),
     ylab=expression(""),
     main="", ylim=c(0,2.5))
lines(theta, full.bayes$dposterior(theta=theta, predict=T), lty="solid")
```

Figure \ref{fig:marginalposteriors} illustrates the marginal posteriors for all model parameters.

```{r, fig.show='hold', out.width='75%', fig.align='center'}
pooledMean <- full.bayes$summary["mean", "mu"]
pooledCRI <- full.bayes$post.interval(mu.level=0.95, individual=F)

x <- 1:8
y <- full.bayes$theta["mean", ]
l <- full.bayes$theta["95% lower", ]
u <- full.bayes$theta["95% upper", ]

plotCI(x, y, li=l, ui=u, ylim=c(-4, 0), xlab="random effect", ylab="equi-tailed 95% CrI", 
       pch=18, main="Posterior equi-tailed 95% CrI of the random effect")
points(x, rep(pooledMean, 8), col="red", pch=7)
abline(h=pooledMean, col="red")
abline(h=pooledCRI[1], col="red", lty=2)
abline(h=pooledCRI[2], col="red", lty=2)
legend("bottomleft", legend=c("Full Bayes", "Pooled"), 
       col=c("black", "red"), pch=c(18, 7), bty="n")
```

The full Bayesian meta-analysis expressed by NNHM provides inference on the random effects $\theta_1,\cdots, \theta_k$ that lies in between the inference provided by two models. It returns a pooled inference, see the red confidence band in the figure above. The pooled model is based on the assumption that the true location of $\theta$ is equivalent for all given studies, so $\theta_1=\cdots= \theta_k$ which equals having a between-study variability $\tau = 0$. This assumption changes the NNHM model defined, due to the fact that random effects model collapses to $\theta$, a normal distribution with location $\theta$ and variance $\tau^2 = 0$. 
The new model then reads:

Likelihood:
$$
y_i\sim\text{N}(\theta_i,\sigma_i^2)
$$
for $i=1, \cdots, k$

Prior:
$$
\theta\sim\text{N}(\nu,\gamma^2)
$$
A sequential application fo the Bayes theorem leads to the posterior:



$$
\theta\mid y_1,..., y_n \sim N\Bigg(\frac{\sum_{i=1}^{k}\frac{y_i}{\sigma^2_i}+\frac{\nu}{\gamma^2}}{\sum_{i=1}^{k}\frac{1}{\sigma^2_i}+\frac{1}{\gamma^2}}, \bigg(\sum_{i=1}^{k}\frac{1}{\sigma^2_i}+\frac{1}{\gamma^2}\bigg)^{-1}\Bigg)
$$

The Figure above shows the results when the in between-study heterogeneity $\tau=0, \nu=0$ and $\gamma=4$. The pooled posterior mean is equal to `r pooledMean` and the pooled standard deviation is equal to `r  full.bayes$summary[4,2]`. 

See pages 88 and 89 of the script [3] for the discussion of the change in the model definition as well as the plots of the credible intervals for empirical Bayes.








# Exercise 4 (Bayesian meta-analysis with JAGS)

The goal of this exercise is to provide an alternative meta-analysis of data based on the file `06worksheet_JAGSextension.R`.

## Meta-analysis based on `06worksheet_JAGSextension.R`

Likelihood:
$$
\begin{aligned}
y_j & \sim \text{Bin}(n_j, p_j) \\
\eta_j & \sim \text{N}(0, 1/\tau_\text{prec}) \\
\end{aligned}
$$
for $i=1, \cdots, k$, where $\tau_\text{prec}=1/\tau^2$

Priors:
$$
\begin{aligned}
\mu & \sim \text{U}(-10, 10) \\
\beta & \sim \text{U}(-10, 10) \\
\tau & \sim \text{U}(0, 10) \\
\end{aligned}
$$

```{r}
pl1.data <- list(
  N = 16, 
  y = c(23., 12., 19., 9., 39., 6., 9., 10., 120., 18., 107., 26., 82., 16., 126., 23.),
  n = c(107., 44., 51., 39., 139., 20., 78., 35., 208., 38., 150., 45., 138., 20., 201., 34.),
  C1 = c(0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.)
)

pl1.params <- c("mu", "beta", "tau", "p1.star", "p2.star")
```

```{r}
pl1_modelString <- "model {
  #	sampling model (likelihood)
  for (j in 1:N)	{
    y[j] ~ dbin(p[j], n[j])
    logit(p[j]) <- mu + beta * C1[j] + eta[j]
    eta[j] ~ dnorm(0, tau.prec)
  
  #	prediction for posterior predictive checks
  y.pred[j] ~ dbin(p[j], n[j])
  PPC[j] <- step(y[j] - y.pred[j]) - 0.5 * equals(y[j], y.pred[j])
  }
  
  #	priors
  mu ~ dunif(-10, 10)
  beta ~ dunif(-10, 10)
  tau ~ dunif(0, 10)
  tau.prec <- 1/tau/tau
  
  #	population effect
  p1 <- 1/(1+exp(-mu)) 
  p2 <- 1/(1+exp(-mu-beta))
  
  #	predictive distribution for new study effect
  eta.star ~ dnorm(0, tau.prec)
  p1.star <- 1/(1+exp(-mu-eta.star))
  p2.star <- 1/(1+exp(-mu-beta-eta.star))
}"

writeLines(pl1_modelString, con="./models/MetaAnalysis.txt")
```

```{r}
# model initiation
rjags.pl1 <- jags.model(
  file = "./models/MetaAnalysis.txt", 
  data = pl1.data,
  n.chains = 4,
  n.adapt = 4000
)

# burn-in
update(rjags.pl1, n.iter = 4000)

# sampling/monitoring
fit.rjags.pl1.coda <- coda.samples(
  model = rjags.pl1, 
  variable.names = pl1.params, 
  n.iter = 50000,
  thin = 5
)

summary(fit.rjags.pl1.coda)
```

```{r, fig.show='hold', out.width='50%'}
m.fit.rjags.pl1.coda <- as.matrix(fit.rjags.pl1.coda)
d.chains <- data.frame(
  iterations = rep(8001:18000, times=4), 
  chains = rep(c("chain1", "chain2", "chain3", "chain4"), each=10000), 
  beta = m.fit.rjags.pl1.coda[, "beta"], 
  mu = m.fit.rjags.pl1.coda[, "mu"], 
  p1.star = m.fit.rjags.pl1.coda[, "p1.star"], 
  p2.star = m.fit.rjags.pl1.coda[, "p2.star"], 
  tau = m.fit.rjags.pl1.coda[, "tau"]
)

ggplot(d.chains, aes(x=iterations, y=beta, color=chains)) + geom_line(alpha=0.5) +
  labs(title="Trace of beta", x="Iterations") + theme_classic()

ggplot(d.chains, aes(x=beta, y=..density..)) +
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) +
  labs(title="Density of beta", y="Density") + theme_classic()

ggplot(d.chains, aes(x=iterations, y=mu, color=chains)) + geom_line(alpha=0.5) +
  labs(title="Trace of mu", x="Iterations") + theme_classic()

ggplot(d.chains, aes(x=mu, y=..density..)) +
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) +
  labs(title="Density of mu", y="Density") + theme_classic()

ggplot(d.chains, aes(x=iterations, y=p1.star, color=chains)) + geom_line(alpha=0.5) +
  labs(title="Trace of p1.star", x="Iterations") + theme_classic()

ggplot(d.chains, aes(x=p1.star, y=..density..)) +
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) +
  labs(title="Density of p1.star", y="Density") + theme_classic()

ggplot(d.chains, aes(x=iterations, y=p2.star, color=chains)) + geom_line(alpha=0.5) +
  labs(title="Trace of p2.star", x="Iterations") + theme_classic()

ggplot(d.chains, aes(x=p2.star, y=..density..)) +
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) +
  labs(title="Density of p2.star", y="Density") + theme_classic()

ggplot(d.chains, aes(x=iterations, y=tau, color=chains)) + geom_line(alpha=0.5) +
  labs(title="Trace of tau", x="Iterations") + theme_classic()

ggplot(d.chains, aes(x=tau, y=..density..)) +
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) +
  labs(title="Density of tau", y="Density") + theme_classic()
```

```{r}
d.summary <- t(rbind(
  colMeans(m.fit.rjags.pl1.coda), 
  apply(m.fit.rjags.pl1.coda, 2, function(x) sd(x)), 
  apply(m.fit.rjags.pl1.coda, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
))

colnames(d.summary) <- c("Mean", "SD", "2.5%", "Median", "97.5%")
knitr::kable(d.summary, align="c", digits=4, caption="Summary statistics for parameters (JAGS)")
```

## Comparison and discussion

**Model (Exercise 1 of Worksheet 5)**

In this model, we first apply the logit-transformation to $p_i=x_i/n_i$ to get an approximately normal distribution of logit-transformed rates. We then use the delta method to compute the standard of logit-transformed rates.

$$
y_i=\text{logit}(p_i)=\log\frac{p_i}{1-p_i}=\log\frac{x_i}{n_i-x_i}
$$

$$
\sqrt{\frac{1}{\tau_i^s}}=\text{SE}(y_i)=\sqrt{\frac{1}{x_i}+\frac{1}{n_i-x_i}}
$$
The full Bayesian meta-analysis is conducted using the Bayesian normal-normal hierarchical model (NNHM) with three levels of hierarchy:

Likelihood:
$$
y_i \sim \text{N}(\theta_i, 1/ \tau^{s}_i)
$$
for $i=1, \cdots, N$

Random effects:
$$
\theta_i \sim \text{N}(\mu, 1/\tau) 
$$

Priors:
$$
\begin{aligned}
  \mu&\sim\text{N}(0,100^2) \\
  \tau&\sim\text{G}(0.001, 0.001)
\end{aligned}
$$

**Model (Exercise 3 of Worksheet 6)**

This model uses the same idea as for the model in Exercise 1 of Worksheet 5. The only difference is that in this model we consider the historical data for both placebo and treatment groups. We first compute the so-called log odds ratio, which is simply the difference between logit-transformed rates in the placebo group and logit-transformed rates in the treatment group. We then use the formula from [Held and Sabanes Bove, 2020, p. 137–138] to compute the standard error of the log odds ratio.

$$
y=\log(\text{OR})=\log\frac{x_\text{P}}{n_\text{P}-x_\text{P}}-
\log\frac{x_\text{T}}{n_\text{T}-x_\text{T}}
$$

$$
\sigma=\text{SE}\left(\log(\text{OR})\right)=
\sqrt{\frac{1}{x_\text{P}} +\frac{1}{n_\text{P}-x_\text{P}} + 
\frac{1}{x_\text{T}} +\frac{1}{n_\text{T}-x_\text{T}}
}
$$

The full Bayesian meta-analysis is conducted using the Bayesian normal-normal hierarchical model (NNHM) with three levels of hierarchy:

Likelihood:
$$
y_i\sim\text{N}(\theta_i,\sigma_i^2)
$$
for $i=1, \cdots, k$

Random effects:
$$
\theta_i\sim\text{N}(\mu,\tau^2)
$$
Priors:
$$
\begin{aligned}
\mu & \sim \text{N}(\nu,\gamma^2) \\
\tau & \sim \lvert \text{N}(0, A^2) \rvert=\text{HN}(A)
\end{aligned}
$$
where $\nu=0, \gamma=4, A=0.5$

**Model (Exercise 4 of Worksheet 6)**

Unlike models stated before, this model uses a linear regression with a normal error ($\eta_j$) to directly model the number of responders with only one predictor indicating whether in the treatment or not.

$$
y_j=\mu+\beta\cdot\text{C1}_j+\eta_j
$$
where $\text{C1}_i$ is a binary variable which is equal to 0 if placebo and 1 otherwise.


Likelihood:
$$
\begin{aligned}
y_j & \sim \text{Bin}(n_j, p_j) \\
\eta_j & \sim \text{N}(0, 1/\tau_\text{prec})
\end{aligned}
$$
for $i=1, \cdots, k$, where $\tau_\text{prec}=1/\tau^2$

Priors:
$$
\begin{aligned}
\mu & \sim \text{U}(-10, 10) \\
\beta & \sim \text{U}(-10, 10) \\
\tau & \sim \text{U}(0, 10) \\
\end{aligned}
$$

# Exercise 5 (Moments of the Poisson-gamma distribution)

Let $Y|\lambda\sim P(\lambda)$ with $\lambda\sim G(\alpha,\beta)$. Use the expressions for iterated expectation

$$
\mathbb{E}(Y)=\mathbb{E}_{\lambda}[\mathbb{E}_{Y}(Y\mid\lambda)] 
$$
and variance (Held and Sabanes Bove, 2020, Section A.3.4)

$$
\text{Var}(Y)=\text{Var}_\lambda[\mathbb{E}_Y(Y\mid\lambda)] + \mathbb{E}_\lambda[\text{Var}_Y(Y\mid\lambda)]
$$
To derive both, the expectation and the variance of the random variable $Y$.

Hints:
Poisson distribution: $X\sim \text{Po}(\lambda): \mathbb{E}(X)=\lambda,\text{Var}(X)=\lambda$

Gamma distribution: $X\sim \text{G}(\alpha,\beta): \mathbb{E}(X)=\alpha/\beta,\text{Var}(X)=\alpha/\beta^2$


Solution:

$$
\begin{aligned}
\mathbb{E}(Y) 
&= \mathbb{E}_{\lambda}[\mathbb{E}_{Y}(Y\mid\lambda)] 
& \because Y\mid\lambda \sim \text{Po}(\lambda) \\
&= \mathbb{E}_{\lambda}(\lambda) 
& \because \lambda \sim \text{G}(\alpha,\beta) \\
&= \frac{\alpha}{\beta}
\end{aligned}
$$

$$
\begin{aligned}
\text{Var}(Y)
&= \text{Var}_\lambda[\mathbb{E}_Y(Y\mid\lambda)] + 
\mathbb{E}_\lambda[\text{Var}_Y(Y\mid\lambda)]
& \because Y\mid\lambda \sim \text{Po}(\lambda) \\
&= \text{Var}_\lambda(\lambda) + \mathbb{E}_\lambda(\lambda) 
& \because \lambda \sim \text{G}(\alpha,\beta) \\
&= \frac{\alpha}{\beta^2} + \frac{\alpha}{\beta} \\
&= \frac{\alpha(1+\beta)}{\beta^2}
\end{aligned}
$$


# Exercise 6 (Empirical Bayes)

Consider observed numbers of lip cancer cases per district for each of 56 districts in Scotland:

```{r}
y <- c(11, 5, 15, 9, 6, 9, 2, 3, 26, 39, 20, 31, 9, 16, 6, 16, 19, 17, 15, 11, 19,
7, 10, 0, 7, 7, 9, 2, 8, 8, 11, 6, 28, 4, 1, 1, 1, 8, 6, 3, 2, 1, 7, 10, 9, 11, 3,
11, 5, 8, 3, 7, 0, 8, 7, 13)
```

Assume that these observations are $i.i.d.$ realizations of the model $Y\mid\lambda\sim \text{Po}(\lambda)$  with $\lambda\sim \text{G}(\alpha,\beta)$. Apply and compare two different approaches to compute empirical Bayes estimates for each district:

## Numerical maximization of log-likelihood

(a) Numerical maximization of the log-likelihood corresponding to the Poisson-gamma distribution as described by (Held and Sabanes Bove, 2020, p. 210) to obtain the marginal maximum likelihood estimator.

$$
\underbrace{f(\lambda\mid y_{1:n})}_\text{Posterior} \propto 
\underbrace{f(y_{1:n}\mid\lambda)}_\text{Likelihood} \cdot 
\underbrace{f(\lambda)}_\text{Prior}
$$

Likelihood:
$$
f(y_{1:n}\mid\lambda)=\prod_{i=1}^{n}\frac{\lambda^{y_i}\exp(-\lambda)}{y_i!}
\propto\lambda^{\sum_{i=1}^{n} y_i}\exp(-n\lambda)
$$

Prior:
$$
f(\lambda)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}\exp(-\beta\lambda)
\propto \lambda^{\alpha-1}\exp(-\beta\lambda)
$$

Posterior:
$$
\begin{aligned}
f(\lambda\mid y_{1:n}) &\propto f(y_{1:n}\mid\lambda)\cdot f(\lambda) \\
&\propto \lambda^{\sum_{i=1}^{n} y_i}\exp(-n\lambda)\cdot 
\lambda^{\alpha-1}\exp(-\beta\lambda) \\
&= \lambda^{\sum_{i=1}^{n} y_i+\alpha-1}\exp(-(n+\beta)\lambda)
\end{aligned}
$$

$$
f(\lambda\mid y_{1:n}) \propto
\lambda^{(\alpha+\sum_{i=1}^{n} y_i)-1}\exp(-(\beta+n)\lambda)
$$
Hence
$$
\lambda\mid y_{1:n}\sim\text{G}\left(\alpha+\sum_{i=1}^{n} y_i, \beta+n \right)
$$

In the empirical Bayes setting, we define the estimates of the prior based on the maximum likelihood estimates of the prior predictive distribution. This is also called the marginal likelihood and in our context has the Poisson-gamma form $y_i\sim \text{PoG}(\alpha,\beta, 1)$ with the log-likelihood

Likelihood:
$$
y_i\mid\lambda\sim\text{Po}(\lambda)
$$

Prior:
$$
\lambda\sim\text{G}(\alpha, \beta)
$$

Prior predictive distribution:
$$
\begin{aligned}
f(y_{i})&=\int_{0}^{\infty}f(y_i\mid\lambda)\cdot f(\lambda)\text{d}\lambda \\
&=\int_{0}^{\infty} \frac{\lambda^{y_i}\exp(-\lambda)}{y_i!}\cdot
\frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha-1}\exp(-\beta\lambda) \text{d}\lambda \\
&=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\cdot\frac{1}{y_i!}
\int_{0}^{\infty} \lambda^{y_i+\alpha-1}\exp(-(1+\beta)\lambda)\text{d}\lambda \\
&=\frac{\beta^{\alpha}}{(\beta+1)^{\alpha+y_i}}\cdot
\frac{\Gamma(\alpha+y_i)}{\Gamma(\alpha)}\cdot\frac{1}{y_i!}
\underbrace{
\int_{0}^{\infty}\frac{(\beta+1)^{\alpha+y_i}}{\Gamma(\alpha+y_i)}
\lambda^{(\alpha+y_i)-1}\exp(-(\beta+1)\lambda)\text{d}\lambda
}_\text{integrates to 1} \\
&=\frac{\beta^{\alpha}}{(\beta+1)^{\alpha+y_i}}\cdot
\frac{\Gamma(\alpha+y_i)}{\Gamma(\alpha)}\cdot\frac{1}{y_i!}
\end{aligned}
$$

Log-likelihood:
$$
\begin{aligned}
l(\alpha,\beta) &= \log \prod_{i=1}^{n}f(y_i) \\
&= \sum_{i=1}^{n}\log f(y_i) \\
&= \sum_{i=1}^{n}\log \left(
\frac{\beta^{\alpha}}{(\beta+1)^{\alpha+y_i}}\cdot
\frac{\Gamma(\alpha+y_i)}{\Gamma(\alpha)}\cdot\frac{1}{y_i!}
\right) \\
&= \sum_{i=1}^{n}\left[
\alpha\log(\beta)-(\alpha+y_i)\log(\beta+1)+
\log\left(\frac{\Gamma(\alpha+y_i)}{\Gamma(\alpha)}\right)-\log(y_i!)
\right] \\
&\propto \sum_{i=1}^{n}\left[\alpha \log(\beta)+\log\left(\frac{\Gamma(\alpha+y_i)}{\Gamma(\alpha)}\right)-(\alpha+y_i)\log(\beta+1)\right]
\end{aligned}
$$

```{r, warning=FALSE}
## Log-likelihood function
ll <- function(x) {
  alpha <- x[1]
  beta <- x[2]
  ll <- sum(alpha * log(beta) + log( (gamma(alpha + y))/(gamma(alpha)) ) - 
              (alpha + y) * log(beta + 1))
  return(ll)
}

## Maximize log-likelihood function
opt <- optim(par=c(0.1, 0.1), fn=ll, control = list(fnscale = -1))

## Print the maximum likelihood estimates
opt$par
```

```{r, warning=FALSE, include=FALSE, eval=FALSE}
## Implement log-likelihood
ll <- function(par, x){
  ll <- sum(dnbinom(x, size = par[1], prob = par[2]/(par[2]+1), log = T))
  return(ll)
}
## Optimizing
opt <- optim(par = c(0.1, 0.1), fn = ll, x = y, method = "BFGS", 
             control = list(fnscale = -1),
             hessian = T)
opt$par
```
Thus, we have $\hat{\alpha}_\text{ML}$ and $\hat{\beta}_\text{ML}$ and can put them into the posterior formula calculated above.

```{r, include=FALSE, eval=FALSE}
# Posterior distribution of lambda
set.seed(34324)
result <- c()
for(i in 1:length(y)){
  lambda_post <- rgamma(n=100000, shape=opt$par[1]+y[i], rate=opt$par[2]+1)
  current_result <- c("Mean"=mean(lambda_post),
                     "CrI:2.5%"=quantile(lambda_post,c(0.025)),
                     "Median"=quantile(lambda_post,c(0.5)),
                     "CrI:97.5%"=quantile(lambda_post,c(0.975))
  )
  result <- rbind(result, current_result)
}
result <- data.frame(result)
rownames(result) <- NULL
colnames(result) <- c("Mean","Lower","Median" ,"Upper")
result$Method <- "EmpiricalBayes"
result$district <- 1:nrow(result)
result$width <-  result$Upper-result$Lower
```

## Machining of moments based on the Exercise 5

(b) Matching of moments based on the Exercise 5 above, which provides the marginal moment estimator.

In the Exercise 5, we have derived:
$$
\begin{aligned}
\mathbb{E}(Y)&=\frac{\alpha}{\beta} \\
\text{Var}(Y)&=\frac{\alpha(1+\beta)}{\beta^2}
\end{aligned}
$$

Let us start with $\text{Var}(Y)$:
$$
\begin{aligned}
\text{Var}(Y)&=\frac{\alpha(1+\beta)}{\beta^2} \\
\text{Var}(Y)&=\mathbb{E}(Y)\cdot \frac{1+\beta}{\beta} \\
\frac{\text{Var}(Y)}{\mathbb{E}(Y)}&= \frac{1}{\beta}+1 \\
\beta&=\frac{1}{\frac{\text{Var}(Y)}{\mathbb{E}(Y)}-1}
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{E}(Y)&=\frac{\alpha}{\beta} \\
\alpha&=\beta\mathbb{E}(Y) \\
&=\frac{\mathbb{E}(Y)}{\frac{\text{Var}(Y)}{\mathbb{E}(Y)}-1}
\end{aligned}
$$

$$
\begin{cases}
\alpha&=\frac{\mathbb{E}(Y)}{\frac{\text{Var}(Y)}{\mathbb{E}(Y)}-1} \\
\beta&=\frac{1}{\frac{\text{Var}(Y)}{\mathbb{E}(Y)}-1}
\end{cases}
$$

```{r}
## Moment-matching function
match.moments <- function(mean, var) {
  alpha <- mean / (var/mean - 1)
  beta <- 1 / (var/mean - 1)
  return(params = c(alpha=alpha, beta=beta))
}

params <- match.moments(mean = mean(y), var = var(y)); params
alpha <- params[1]
beta <- params[2]
```

```{r, include=FALSE, eval=FALSE}
## Posterior distribution of lambda
set.seed(34324)
result_MaMo <- c()
for(i in 1:length(y)){
  lambda_post <- rgamma(n=100000, shape=alpha+y[i], rate=beta+1)
  current_result <- c("Mean"=mean(lambda_post),
                     "CrI:2.5%"=quantile(lambda_post,c(0.025)),
                     "Median"=quantile(lambda_post,c(0.5)),
                     "CrI:97.5%"=quantile(lambda_post,c(0.975))
  )
  result_MaMo <- rbind(result_MaMo, current_result)
}
result_MaMo <- data.frame(result_MaMo)
rownames(result_MaMo) <- NULL
colnames(result_MaMo) <- c("Mean","Lower","Median" ,"Upper")
result_MaMo$Method <- "MOM"
result_MaMo$district <- 1:nrow(result_MaMo)
result_MaMo$width <-  result_MaMo$Upper-result_MaMo$Lower
```

## Comparison and discussion

Compare means and the lengths of equi-tailed 95\%CrI obtained by both approaches. Report your results

```{r, eval=FALSE, include=FALSE}
set.seed(44566)
M <- 100000
column.names <- c("District", "Mean", "Lower", "Median", "Upper", "Length", "Method")
results.eb <- data.frame(matrix(nrow=length(y), ncol=7))
results.mm <- data.frame(matrix(nrow=length(y), ncol=7))
colnames(results.eb) <- column.names
colnames(results.mm) <- column.names

for (i in 1:length(y)) {
  lambda.eb <- rgamma(n=M, shape=opt$par[1]+y[i], rate=opt$par[2]+1)
  results.eb[i, 1] <- i
  results.eb[i, 2] <- mean(lambda.eb)
  results.eb[i, 3:5] <- quantile(lambda.eb, probs=c(0.025, 0.5, 0.975))
  results.eb[i, 6] <- results.eb[i, 5] - results.eb[i, 3]
  results.eb[i, 7] <- "Empirical Bayes"
  
  lambda.mm <- rgamma(n=M, shape=alpha+y[i], rate=beta+1)
  results.mm[i, 1] <- i
  results.mm[i, 2] <- mean(lambda.mm)
  results.mm[i, 3:5] <- quantile(lambda.mm, probs=c(0.025, 0.5, 0.975))
  results.mm[i, 6] <- results.mm[i, 5] - results.mm[i, 3]
  results.mm[i, 7] <- "Moment Matching"
}
```


```{r}
column.names <- c("District", "Mean", "Lower", "Median", "Upper", "Length", "Method")
df1 <- data.frame(matrix(nrow=length(y), ncol=7))
df2 <- data.frame(matrix(nrow=length(y), ncol=7))
colnames(df1) <- column.names
colnames(df2) <- column.names

for (i in 1:length(y)) {
  ## Empirical Bayes
  df1[i, 1] <- i
  df1[i, 2] <- (opt$par[1]+y[i]) / (opt$par[2]+1)
  df1[i, 3:5] <- qgamma(c(0.025, 0.5, 0.975), shape=opt$par[1]+y[i], rate=opt$par[2]+1)
  df1[i, 6] <- df1[i, 5] - df1[i, 3]
  df1[i, 7] <- "Empirical Bayes"
  
  ## Moment Matching
  df2[i, 1] <- i
  df2[i, 2] <- (alpha+y[i]) / (beta+1)
  df2[i, 3:5] <- qgamma(c(0.025, 0.5, 0.975), shape=alpha+y[i], rate=beta+1)
  df2[i, 6] <- df2[i, 5] - df2[i, 3]
  df2[i, 7] <- "Moment Matching"
}
```

```{r, fig.show='hold', out.width='75%', fig.align='center'}
d.plot <- rbind(df1, df2)
ggplot(data=d.plot, aes(x=District, y=Mean, color=Method)) + 
  geom_point(position=position_dodge(.9)) + 
  geom_errorbar(aes(ymin=Lower, ymax=Upper), width=.9, position=position_dodge(0.9)) + 
  # scale_color_manual(values = c(3, 4)) + 
  labs(title = "Posterior distribution of Lambda", 
       y=expression(lambda[i]), x="District") + theme_classic()
```

```{r, fig.show='hold', out.width='75%', include=FALSE, eval=FALSE}
d.plot <- rbind(results.eb, results.mm)
ggplot(data=d.plot, aes(x=District, y=Mean, color=Method)) + 
  geom_point(position=position_dodge(.9)) + 
  geom_errorbar(aes(ymin=Lower, ymax=Upper), width=.9, position=position_dodge(0.9)) + 
  # scale_color_manual(values = c(3, 4)) + 
  labs(title = "Posterior distribution of Lambda", 
       y=expression(lambda[i]), x="District") + theme_classic()
```


```{r, include=FALSE, eval=FALSE}
results_comp <- rbind(result, result_MaMo)
ggplot(data=results_comp, aes(x=district, y=Mean, col=Method)) + 
  geom_point(position=position_dodge(.9)) + 
  geom_errorbar(aes(ymin=Lower, ymax=Upper), position=position_dodge()) + 
  labs(title = "Posterior distribution of Lambda", 
       y=expression(lambda[i]), x="Districts") + theme_minimal()
```


```{r, eval=FALSE, include=FALSE}
mean.diff <- results.eb$Mean - results.mm$Mean
length.diff <- results.eb$Length - results.mm$Length

d.comparison <- rbind(
  c("Mean"=mean(mean.diff), quantile(mean.diff, probs=c(0.025,0.5,0.975))), 
  c("Mean"=mean(length.diff), quantile(length.diff, probs=c(0.025,0.5,0.975)))
)

rownames(d.comparison) <- c("Mean difference", "Length difference")
knitr::kable(d.comparison, align="c", digits=4, caption="Comparison of the two methods")
```



```{r}
mean.diff <- df1$Mean - df2$Mean
length.diff <- df1$Length - df2$Length

d.comparison <- rbind(
  c("Mean"=mean(mean.diff), quantile(mean.diff, probs=c(0.025,0.5,0.975))), 
  c("Mean"=mean(length.diff), quantile(length.diff, probs=c(0.025,0.5,0.975)))
)

rownames(d.comparison) <- c("Mean difference", "Length difference")
knitr::kable(d.comparison, align="c", digits=4, caption="Comparison of the two methods")
```

The Moment Matching method yields in general higher values and the width of the confidence intervals tends to be larger.



\newpage

# References

[1] Baeten, D., X. Baraliakos, J. Braun, J. Sieper, P. Emery, D. van der Heijde, I. McInnes, J. van Laar,
R. Landewé, P. Wordsworth, J. Wollenhaupt, H. Kellner, J. Paramarta, J. Wei, A. Brachat, S. Bek,
D. Laurent, Y. Li, Y. Wang, A. Bertolino, S. Gsteiger, A. Wright, and W. Hueber (2013). Antiinterleukin-
17A monoclonal antibody secukinumab in treatment of ankylosing spondylitis: a
randomised, double-blind, placebo-controlled trial. The Lancet 382, 1705–1713.

[2] Held, L. and D. Sabanés Bové (2020). Likelihood and Bayesian Inference: With Applications in Biology and
Medicine. Springer.

[3] Roos, M. (2022). Foundations of Bayesian Methodology: FS22, Department of Biostatistics at Epidemiology, Biostatistics and Prevention Institute, University of Zurich

[4] Röver, C. (2020). Bayesian random-effects meta-analysis using the bayesmeta R package. Journal of
Statistical Software 93(6), 1–51.

