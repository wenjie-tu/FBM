---
title: "Worksheet 5"
author: 
  - Wenje Tu
  - Lea Bührer
  - Jerome Sepin
  - Zhixuan Li
  - Elia-Leonid Mastropietro
  - Jonas Raphael Füglistaler
date: "Spring Semester 2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
# bibliography: biblio.bib
# nocite: '@*'
subtitle: Foundations of Bayesian Methodology
papersize: a4
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang="us_en")
rm(list=ls())
```


# Exercise 3 - Normal example in JAGS

**Confused finding/declaration**: although `set.seed(44566)` is implemented in every chunk that will generate random number, the results still change every time. That is the reason why the results and interpretation are NOT consistent with compiled version.

## rjags interface to JAGS
The steps of `list.factories` and `set.factories` show and control over the status of factories in JAGS modules. The steps of `jags.modules` shows the names of the currently loaded modules and also loads or unloads JAGS modules. 
```{r rjags, warning=FALSE, out.width="70%"}
remove(list=ls())
set.seed(44566)
# set the path to the 05normal_exmple_JAGS.txt file
path <- ""

suppressPackageStartupMessages(library(rjags))
list.factories(type = "rng")
list.factories(type = "monitor")
list.factories(type = "sampler")
set.factory(name = "base::Slice", type = "sampler", state = FALSE)
list.factories(type = "sampler")
set.factory(name = "base::Slice", type = "sampler", state = TRUE)
list.factories(type = "sampler")
list.modules()
load.module("glm")
list.modules()
unload.module("glm")
list.modules()
```

### Introduction
The basic setup of normal example is as following codes
```{r introduction, fig.align='center', warning=FALSE, out.width="40%"}
y <- c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
       9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
       -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
       11.144,5.564)
par(mfrow = c(1, 1))
boxplot(y)
summary(y)
sd(y)
# Define the parameters of the prior distributions 
mu0 <- -3
sigma2_0 <- 4
a0 <- 1.6
b0 <- 0.4
```

Boxplot shows that the median of observed data is around 3, the 25% quantile and 75% quantile are around 0 and 5, respectively. In addition, the minimum and maximum are around -5 and 11, respectively.

###  INLA exact result (motivation)
```{r INLA, warning=FALSE, out.width="70%"}
suppressPackageStartupMessages(library(INLA))
formula <- y ~ 1
inla.output <- inla(formula,data=data.frame(y=y),
                    control.family = list(hyper =
                                            list(prec = list(prior="loggamma",
                                                             param=c(a0,b0)))),
                    control.fixed = list(mean.intercept=mu0, 
                                         prec.intercept=1/sigma2_0))
```

## Step 2: JAGS model file as a string in rjags with coda
This step gives observed values, sets up initial values and specifies model.
```{r, warning=FALSE, out.width="70%"}
set.seed(44566)
suppressPackageStartupMessages(library(rjags))
suppressPackageStartupMessages(library(coda))
wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)
wb_inits <- list( mu=-0.2381084, inv_sigma2=0.3993192 )
modelString = " # open quote for modelString
model{
# likelihood
for (i in 1:N){ 
y[i] ~ dnorm( mu, inv_sigma2 )    
}
# Priors
mu ~ dnorm( -3, 0.25 ) # prior for mu N(mu0, prec=1/sigma2_0)
inv_sigma2 ~ dgamma( 1.6, 0.4 ) # prior for precision G(a0, b0)

# transformations
# deterministic definition of variance
sigma2 <- 1/inv_sigma2

# deterministic definition of standard deviation
sigma <- sqrt(sigma2)
}
" # close quote for modelString
writeLines(modelString, con="TempModelexe3.txt") # write to a file
```

## JAGS only one chain
```{r one chain, fig.align='center', warning=FALSE, out.width="70%"}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(FNN))
# model initiation
set.seed(44566)
model.jags <- jags.model(
  file = "TempModelexe3.txt", 
  data = wb_data,
  inits = wb_inits,
  n.chains = 1,
  n.adapt = 4000
)
str(model.jags)
class(model.jags)
attributes(model.jags)
list.samplers(model.jags)
update(model.jags, n.iter = 4000) # burn-in
# sampling
fit.jags.coda <- coda.samples(
  model = model.jags, 
  variable.names = c("mu", "sigma2", "inv_sigma2"), 
  n.iter = 10000,
  thin = 1
)
str(fit.jags.coda)
class(fit.jags.coda)
attributes(fit.jags.coda)
summary(fit.jags.coda)
plot(fit.jags.coda)
# store samples for each parameter from the chain into separate objects
m.fit.jags.coda <- as.matrix(fit.jags.coda)
mu.sim <- m.fit.jags.coda[,"mu"] 
sigma2.sim <- m.fit.jags.coda[,"sigma2"]
inv_sigma2.sim <- m.fit.jags.coda[,"inv_sigma2"] 

par(mfrow=c(2,2))
# plot for mean
rg <- range(inla.output$marginals.fixed$"(Intercept)"[,2])
truehist(mu.sim, prob=TRUE, col="yellow", xlab=expression(mu),ylim=rg)
lines(density(mu.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),
       lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")
KL.divergence(inla.rmarginal(length(mu.sim), inla.output$marginals.fixed[[1]]), 
              mu.sim, k = 1)
# plot for variance
m_var <-inla.tmarginal(function(x) 1/x, inla.output$marginals.hyperpar[[1]])
rg <- range(m_var[,2])
truehist(sigma2.sim, prob=TRUE, col="yellow", xlab=expression(sigma^2),ylim=rg)
lines(density(sigma2.sim),lty=3,lwd=3, col=2)
lines(m_var,lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),
       cex=1.0,bty="n")
# plot for precision
truehist(inv_sigma2.sim, prob=TRUE, col="yellow", xlab=expression(1/sigma^2))
lines(density(inv_sigma2.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.hyperpar[[1]],lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),
       cex=1.0,bty="n")
KL.divergence(inla.rmarginal(length(inv_sigma2.sim), 
                             inla.output$marginals.hyperpar[[1]]), 
              inv_sigma2.sim, k = 1)
```

The empirical mean and standard deviation for each variable and corresponding quantiles are shown in the output.
The traceplot as well as the plot of density indicate good mixing for three different parameters: precision, mean and variance. When compared to the result of INLA, whether plots or the results of Kullback-Leibler divergence proves the similarity between the output of INLA and JAGS.

## JAGS several chains
```{r chains, fig.align='center', warning=FALSE, out.width="70%"}
set.seed(44566)
wb_inits <- function() {
  list(mu = rnorm(1),
       inv_sigma2 = runif(1)
  )  
}
# model initialisation
model.jags <- jags.model(
  file = "TempModelexe3.txt", 
  data = wb_data,
  inits = wb_inits,
  n.chains = 4,
  n.adapt = 4000
)
update(model.jags, n.iter = 4000) # burn-in
# sampling/monitoring
fit.jags.coda <- coda.samples(
  model = model.jags, 
  variable.names = c("mu", "sigma2", "inv_sigma2"), 
  n.iter = 10000,
  thin = 10
)
summary(fit.jags.coda)
plot(fit.jags.coda)
# store samples for each parameter from the chains into separate vectors
m.fit.jags.coda <-as.matrix(fit.jags.coda)
mu.sim <- m.fit.jags.coda[,"mu"] 
sigma2.sim <- m.fit.jags.coda[,"sigma2"]
inv_sigma2.sim <- m.fit.jags.coda[,"inv_sigma2"] 
par(mfrow=c(2,2))
# plot for mean
rg <- range(inla.output$marginals.fixed$"(Intercept)"[,2])
truehist(mu.sim, prob=TRUE, col="yellow", xlab=expression(mu),ylim=rg)
lines(density(mu.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),
       cex=1.0,bty="n")
KL.divergence(inla.rmarginal(length(mu.sim), inla.output$marginals.fixed[[1]]), 
              mu.sim, k = 1)
# plot for variance
m_var <-inla.tmarginal(function(x) 1/x, inla.output$marginals.hyperpar[[1]])
rg <- range(m_var[,2])
truehist(sigma2.sim, prob=TRUE, col="yellow", xlab=expression(sigma^2),ylim=rg)
lines(density(sigma2.sim),lty=3,lwd=3, col=2)
lines(m_var,lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),
       cex=1.0,bty="n")
# plot for precision
truehist(inv_sigma2.sim, prob=TRUE, col="yellow", xlab=expression(1/sigma^2))
lines(density(inv_sigma2.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.hyperpar[[1]],lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),
       cex=1.0,bty="n")
KL.divergence(inla.rmarginal(length(inv_sigma2.sim), 
                             inla.output$marginals.hyperpar[[1]]), 
              inv_sigma2.sim, k = 1)
```

The empirical mean and standard deviation for each variable and corresponding quantiles are shown in the output.
The result of four chains is similar as the result of one chain: From traceplot and the plot of density, we can find good mixing for three parameters; INLA and JAGS have similar results.

### CODA
```{r coda, fig.align='center', warning=FALSE, out.width="70%"}
effectiveSize(fit.jags.coda)
lapply(fit.jags.coda, effectiveSize)
gelman.diag(fit.jags.coda,autoburnin=TRUE)
gelman.plot(fit.jags.coda,autoburnin=TRUE)
geweke.diag(fit.jags.coda)
geweke.plot(fit.jags.coda)
heidel.diag(fit.jags.coda)
raftery.diag(fit.jags.coda)

par(mfrow=c(2,2))
coda:::traceplot(fit.jags.coda)

# "DIC" penalised expected deviance computation
dic1<-dic.samples(model=model.jags, n.iter=1000, type="popt")
```

**ESS**: The Effective Sample Size of three parameters are shown in the table. The ESS must be large enough to get stable inferences for quantities of interest. In the case of out of 10000 iterations, the global ESS around 4000 are large enough. 


\begin{table}[h]
\centering
\begin{tabular}{clllll}
\hline
          & Global   & 1st Chain & 2nd Chain & 3rd Chain & 4th Chain \\ \hline
Precision & 3863.425 & 981.847   & 1000.000  & 881.578   & 1000      \\
Mean      & 4099.911 & 1199.804  & 900.106   & 1000.000  & 1000      \\
Variance  & 3931.376 & 931.376   & 1000.000  & 1000.000  & 1000      \\ \hline
\end{tabular}
\caption{Effective Sample Size}
\label{tab:ESS}
\end{table}

**Gelman-Rubin-Brooks**: The estimated shrink factors and corresponding upper CI of three parameters of Gelman and Rubin's convergence diagnostic are within the acceptable range. In addition, Gelman-Rubin-Brooks plot shows the evolution of shrink factor as the number of iterations increases. The trends of the median of shrink factors are mostly stable among three parameters in the range of 6000 to 14000 iterations. 

**Geweke**: The results of Geweke's convergence diagnostic for 4 chaines are all in the range of -2 to 2. However, the Geweke-Brooks plot indicates that the 4th chain has more samples who holds a Z-score that out of the range from -2 to 2, which proves that the 4th chain has not reached equilibrium.

**Heidelberger & Welch**: Heidelberger and Welch's convergence diagnostic shows a similar result as Geweke-Brooks plot. The parameter of variance in the 4th chain fails stationary test indicating non convergence therefore a longer MCMC run is needed. Except of this parameter, all parameters also pass the halfwidth test that means the length of the sample is long enough to estimate the mean with sufficient accuracy

**Raftery & Lewis** Raftery and Lewis's diagnostic gives the he minimum sample size based on zero autocorrelation. All the four chains need a minimum sample size of 3746.

**Trace plot** The traceplots show agian that all the four chains of three parameters hold good mixing.

## Additional sampling in several chains, preparation for BGR/Gelman with runjags
### runjags interface with a link to a file

```{r additional1, fig.align='center', warning=FALSE, out.width="70%"}
suppressPackageStartupMessages(library(runjags))
set.seed(44566)
wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)
wb_inits <- function() {
  list(mu = rnorm(1),
       inv_sigma2 = runif(1)
  )  
}
fit.runjags<-run.jags(model=paste(path,"05normal_exmple_JAGS.txt",sep=""),
                      monitor=c("mu", "sigma2", "inv_sigma2"),
                      data=wb_data,
                      inits=wb_inits,
                      n.chains=4,
                      burnin=4000,
                      sample=5000,
                      adapt=1000,
                      thin=2)
plot(fit.runjags)
print(fit.runjags)
# CODA
fit.runjags.coda<-as.mcmc.list(fit.runjags)
summary(fit.runjags.coda)
# conduct CODA
```

Using `plot` function except ordinary plots that appeared before, autocorrelation plots and a cross-correlation plot are produced.

**Autocorrelation plots** give an overview of autocorrelation in different thinning interval. For all three parameters, when the lag is larger than 1, the autocorrelation is close to 0.

**Cross-correlation plot** shows the correlation between parameters. The correlation between variance and mean is close to 0, while variance and precision hold an inverse correlation.

### R2jags wrapper to rjags interface to JAGS several chains
```{r additional2, fig.align='center', warning=FALSE, out.width="40%"}
suppressPackageStartupMessages(library(R2jags))
set.seed(44566)
wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)
#define parameters
params<-c("mu", "sigma2", "inv_sigma2")
# define inits
inits1 <- list(mu=rnorm(1), inv_sigma2=runif(1),
               .RNG.name="base::Super-Duper", .RNG.seed=1)
inits2 <- list(mu=rnorm(1), inv_sigma2=runif(1),
               .RNG.name="base::Wichmann-Hill", .RNG.seed=2)
wb_inits <- list(inits1,inits2)
fit.R2jags<-jags(data=wb_data,
                 inits=wb_inits,
                 parameters.to.save=params,
                 model.file="05normal_exmple_JAGS.txt",
                 n.chains=2,
                 n.iter=50000,
                 n.burnin=4000,
                 n.thin=5,
                 DIC = TRUE,
                 jags.seed = 321,
                 refresh =100,
                 digits = 4,
                 jags.module = c("glm","dic"))
# Standard plots of the monitored variables
plot(fit.R2jags)
# Display summary statistics
print(fit.R2jags)
# traceplot
traceplot(fit.R2jags)
# CODA
fit.R2jags.coda<-as.mcmc(fit.R2jags)
summary(fit.R2jags.coda)
# conduct CODA
```

The function `R2jags::jags` compute deviance when `DIC` is `true`, where DIC is an estimate of expected predictive error and lower deviance is better. Besides, using `plot` function, the quantiles for each variable are visualized. 

# Exercise 4 - Logistic regression in JAGS

## Bayesian inference

```{r}
x <- c(0.0028, 0.0028, 0.0056, 0.0112, 0.0225, 0.0450)
# the centered covariate values (centered dose) from the Mice data from Collett
x_centered <- x - mean(x)
# number of mice deaths
# y <- c(35, 21, 9, 6, 1)
y <- c(26, 9, 21, 9, 6, 1)
# total number of mice
# n <- c(40, 40, 40, 40, 40)
n <- c(28, 12, 40, 40, 40, 40)
```

```{r mice-table, results='asis'}
d.mice <- data.frame(
  x, y, n, x_centered, y/n, n-y
)
colnames(d.mice) <- c("$x$", "$y$", "$n$", "centered $x$", "$p$", "$alive$")
knitr::kable(d.mice, align="c", caption="Mice data from Collett (2003)")
```

Logistic model:

$$
\text{logit}(p_i)=\ln\left(\frac{p_i}{1-p_i} \right)=\alpha+\beta x_i
$$

$$
p_i=\frac{\exp(\alpha+\beta x_i)}{1+\exp(\alpha+\beta x_i)}
$$

```{r, eval=FALSE, include=FALSE}
## Disaggregate the data
d.mice1 <- data.frame(Response=rep(c(1, 0), c(sum(y), sum(n)-sum(y))), 
                      CenteredDose=c(rep(round(x_centered, 5), y), 
                                     rep(round(x_centered, 5), n-y)))

knitr::kable(table(d.mice1))
```

```{r, fig.align='center', out.width="75%", eval=FALSE, include=FALSE}
mosaicplot(CenteredDose ~ Response, data=d.mice1, color=2:3, 
           main="Mosaic Plot", xlab="Centered Dosage")
```

```{r}
modelString <- "model{
  for (i in 1:length(y)) {
    y[i] ~ dbin(p[i],n[i])
    p[i] <- ilogit(alpha + beta * x[i])
  }
  
  alpha ~ dnorm(0, 1.0E-04)
  beta ~ dnorm(0, 1.0E-04)
}"

writeLines(modelString, con="LogitModel.txt")
```

```{r, message=F, warning=F}
library(rjags)
library(coda)
library(ggplot2)
```

```{r}
## Generate data list for JAGS
dat.jags <- list(y=y, x=x_centered, n=n)

## Initialize starting points (let JAGS initialize) and set seed 
inits.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159),
                   list(.RNG.name="base::Marsaglia-Multicarry", .RNG.seed=159314),
                   list(.RNG.name="base::Super-Duper", .RNG.seed=413159),
                   list(.RNG.name="base::Mersenne-Twister", .RNG.seed=143915))

## Compile JAGS model
model1.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  inits = inits.jags, 
  n.chains = 4,
  n.adapt = 4000
)
```

```{r}
## Burn-in
update(model1.jags, n.iter = 4000)

## Sampling
fit1.jags.coda <- coda.samples(
  model = model1.jags, 
  variable.names = c("alpha", "beta"), 
  n.iter = 10000,
  thin = 1
)
```

```{r}
summary(fit1.jags.coda)
```


```{r, eval=FALSE, include=FALSE}
plot(fit1.jags.coda)
```


```{r, fig.show='hold', out.width="50%"}
m.fit1.jags.coda <-as.matrix(fit1.jags.coda)
d.chains <- data.frame(
  iterations = rep(8001:18000, times=4), 
  alpha = m.fit1.jags.coda[, "alpha"], 
  beta = m.fit1.jags.coda[, "beta"], 
  chains = rep(c("chain1", "chain2", "chain3", "chain4"), each=10000), 
  alphaRanks = rank(m.fit1.jags.coda[, "alpha"]), 
  betaRanks = rank(m.fit1.jags.coda[, "beta"])
)

ggplot(d.chains, aes(x=iterations, y=alpha, color=chains)) + geom_line(alpha=0.5) + 
  labs(title="Trace of alpha", x="Iterations") + theme_minimal()

ggplot(d.chains, aes(x=alpha, y=..density..)) + 
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) + 
  labs(title="Density of alpha", y="Density") + theme_minimal()

ggplot(d.chains, aes(x=iterations, y=beta, color=chains)) + geom_line(alpha=0.5) + 
  labs(title="Trace of beta", x="Iterations") + theme_minimal()

ggplot(d.chains, aes(x=beta, y=..density..)) + 
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) + 
  labs(title="Density of beta", y="Density") + theme_minimal()
```

```{r}
d.summary <- t(rbind(
  colMeans(m.fit1.jags.coda), 
  apply(m.fit1.jags.coda, 2, function(x) sd(x)), 
  apply(m.fit1.jags.coda, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
))

colnames(d.summary) <- c("Mean", "SD", "2.5%", "Median", "97.5%")
knitr::kable(d.summary, align="c", caption="Summary statistics Bayesian approach")
```

## Classic Logistic regression

```{r}
fit_glm_classic <- glm(cbind(y, (n-y)) ~ x_centered, data = d.mice, family = binomial)
summary(fit_glm_classic)
```


# Exercise 5 - CODA for logistic regression in JAGS

```{r, warning=FALSE, message=FALSE}
library(bayesplot)
library(stableGR)
```

```{r rankplot, fig.show="hold", out.width="80%", fig.cap="Rank Plot.", fig.align='center'}
mcmc_rank_hist(fit1.jags.coda)
```

```{r, fig.show='hold', out.width="25%"}
## Rank plot of alpha
ggplot(d.chains[1:10000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=2, fill=2, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain1)") + theme_minimal()

ggplot(d.chains[10001:20000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=3, fill=3, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain2)") + theme_minimal()

ggplot(d.chains[20001:30000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=4, fill=4, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain3)") + theme_minimal()

ggplot(d.chains[30001:40000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=5, fill=5, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain4)") + theme_minimal()

## Rank plot of beta
ggplot(d.chains[1:10000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=2, fill=2, alpha=0.5) + 
  labs(title="Rank plot of beta (chain1)") + theme_minimal()

ggplot(d.chains[10001:20000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=3, fill=3, alpha=0.5) + 
  labs(title="Rank plot of beta (chain2)") + theme_minimal()

ggplot(d.chains[20001:30000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=4, fill=4, alpha=0.5) + 
  labs(title="Rank plot of beta (chain3)") + theme_minimal()

ggplot(d.chains[30001:40000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=5, fill=5, alpha=0.5) + 
  labs(title="Rank plot of beta (chain4)") + theme_minimal()
```

The figure illustrates the histograms of the ranked posterior drawn, ranked over all chains for $\alpha$ and $\beta$. Whereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.
See Vehtari et al. (2019) for details.

It can be seen in the figure that the rank histograms nearly follow a normal distribution.

## Convergence diagnostics

### Heidelberger \& Welch (Convergence to stationarity)

```{r}
heidel.diag(fit1.jags.coda)
```

The output from `heidel.diag` shows the summary results for the four generated chains.
The daignostic on one hand checks if the length of the sample is long enough and on the other hand checks if the means are estimated from a converged chain. 

The stationarity test uses the Cramer-von-Mises statistic to test the null hypothesis that the sampled values come from a stationary distribution. The half-width test calculates a 95% confidence interval for the mean, using the portion of the chain which passed the stationarity test. 

We see that both tests are passed for the four chains and all $p$-values are larger than 0.05 = $\alpha$. We conclude that the sampled values come from a stationary distribution and the length of the sample is considered long enough to estimate the mean with sufficient accuracy.


###  Raftery \& Lewis (Convergence to ergodic average)

```{r}
raftery.diag(fit1.jags.coda)
```

The output from `raftery.diag` shows the summary results for the four generated chains. `raftery.diag` is a run length control diagnostic based on a criterion of accuracy of estimation of the quantile $q$. The dependence factor $I$ ($I=\frac{M+N}{N_\text{min}}$), indicates to which extent autocorrelation inflates the required sample size. $I>5$ indicates strong autocorrelation which may be due to a poor choice of starting value, high posterior correlations or "stickiness" of the MCMC algorithm.  We see that the dependence factors for the four chains are all smaller than 5 and hence no strong autocorrelation exists. 


### Geweke (Convergence to stationarity)

```{r}
geweke.diag(fit1.jags.coda)
```

The output from `geweke.diag` shows the summary results for the four generated chains. `geweke.diag` is a convergence diagnostic for Markov chains based on a test for equality of the means of the first and last part of a Markov chain (by default the first 10% and the last 50%). If the samples are drawn from the stationary distribution of the chain, the two means are equal and Geweke's statistic has an asymptotically standard normal distribution. The test statistic is a standard Z-score.

The idea behind the Geweke's diagnostic is that in a long enough chain whose trace plots suggest convergence to the target distribution, we assume the second half of the chain has converged to the target distribution and we test if the first 10% can be treated as burn-in. So we mimic the simple two-sample test of means: if the mean of the first 10% is not significantly different from the last 50%, then we conclude the target distribution converged somewhere in the first 10% of the chain. So we'll use this 10% as burn-in. 

We see that for the four chains the absolute values of Z-scores for variables alpha and beta are smaller than 2, which indicates that the equilibrium may have been reached. 

```{r,  fig.show="hold", out.width="50%", fig.cap="Geweke Plot."}
geweke.plot(fit1.jags.coda)
```

If `geweke.diag` indicates that the first and last part of a sample from a Markov chain are not drawn from the same distribution, it may be useful to discard the first few iterations to see if the rest of the chain has "converged". `geweke.plot` shows what happens to Geweke's Z-score when successively larger numbers of iterations are discarded from the beginning of the chain. To preserve the asymptotic conditions required for Geweke's diagnostic, the plot never discards more than half the chain. For beta (chain1), alpha (chain2), and alpha (chain3), there exist several segments out of the 95% confidence bands ($\lvert Z \rvert > 2$). 
The plot shows that we mainly obtain z-score values below an absolute value of 2. So we can assume that the chain has converged. 


### Gelman and Rubin's convergence diagnostic

```{r}
gelman.diag(fit1.jags.coda, autoburnin=TRUE)
```

The idea of Gelman and Rubin's convergence diagnostic is to run multiple chains from widely dispersed starting values and perform an Analysis of Variance to see if the between-chain variability ($B$) is large in relation to the average variability within ($W+B$) the pooled chain. 

$$
\sqrt{\hat{R}}\approx
\sqrt{\frac{W+B}{W}}
$$

$\hat{R}$ is so-called "potential scale reduction factor" and it is calculated for each variable. We see that the "Potential scale reduction factors" for both alpha and beta are close to 1, which indicates that the between-chain variability ($B$) is close to 0. In other words, the separate four chains have mixed quite well.

```{r, fig.align='center', fig.dim="70%", fig.cap="Gelman plot."}
gelman.plot(fit1.jags.coda, autoburnin=TRUE)
```

The `gelman.plot` shows that the evolution of Gelman and Rubin's shrink factor as the number of iterations increases. We see that the shrink factors for both variables quickly decrease to 1 after 10000 iterations and the variability of shrink factors becomes more stable as the number of iterations keeps increasing.

###  Gelman-Rubin diagnostic using stable variance estimators

```{r}
stable.GR(fit1.jags.coda)
```

`stable.GR` extends Gelman-Rubin diagnostic using stable variance estimators. We see that the univariate potential scale reduction factors for both alpha and beta are calculated and they are close to 1. Multivariate `psrf` is also calculated by taking into account the interdependence of the chain's components. The PSRFs decrease to 1 as the chain length increases. When the PSRF becomes sufficiently close to 1, the sample collected by the Markov chain has converged to the target distribution. Means of variables (alpha and beta) and the effective sample size are also reported in the output. 

### Conclusions:

- Heidel: We conclude that the sampled values come from a stationary distribution and the length of the sample is considered long enough to estimate the mean with  sufficient accuracy.

- Raftery \& Lewis: The dependence factors for the four chains are all smaller than 5 and hence no strong autocorrelation exists. 

- Geweke: We assume that the equilibrium has been reached. We achieve convergence towards the target distribution.

- Gelman \& Rubin: The between-chain variability is close to 0. The separate four chains have mixed quite well. 

- Gelman \& Rubin for stable variance: The chains have converged to the target distribution.


All in all, it can be concluded that no issues arose during the diagnostics. 



## Re-run the MCMC simulation

In the following steps the MCMC simulation is adapted to the findings from subtask 5.2.
We keep the same model as before but we change n.iter from 10'000 to 30'000 and n.thin from 1 to 3. The number of burn-in iterations (4000) is kept. **WHY do we change to this values???? the diagnostics do not really get better... (excluding the rank_hist)**


```{r}
## Compile JAGS model
model2.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  inits = inits.jags, 
  n.chains = 4,
  n.adapt = 4000
)
```

```{r}
## Burn-in (increase burn-in iterations)
update(model2.jags, n.iter = 4000)

## Initialize starting points (let JAGS initialize) and set seed 
inits.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159),
                   list(.RNG.name="base::Marsaglia-Multicarry", .RNG.seed=159314),
                   list(.RNG.name="base::Super-Duper", .RNG.seed=413159),
                   list(.RNG.name="base::Mersenne-Twister", .RNG.seed=143915))

## Sampling
fit2.jags.coda <- coda.samples(
  model = model2.jags, 
  variable.names = c("alpha", "beta"), 
  n.iter = 30000,
  thin = 3
)
```

```{r}
summary(fit2.jags.coda)
```

```{r, fig.show="hold", out.width="80%", fig.cap="Trace and density plot.", fig.align='center', eval=FALSE, include=FALSE}
plot(fit2.jags.coda)
```

```{r, fig.show='hold', out.width="50%"}
m.fit2.jags.coda <-as.matrix(fit2.jags.coda)
d.chains <- data.frame(
  iterations = rep(8001:18000, times=4), 
  alpha = m.fit2.jags.coda[, "alpha"], 
  beta = m.fit2.jags.coda[, "beta"], 
  chains = rep(c("chain1", "chain2", "chain3", "chain4"), each=10000), 
  alphaRanks = rank(m.fit2.jags.coda[, "alpha"]), 
  betaRanks = rank(m.fit2.jags.coda[, "beta"])
)

ggplot(d.chains, aes(x=iterations, y=alpha, color=chains)) + geom_line(alpha=0.5) + 
  labs(title="Trace of alpha", x="Iterations") + theme_minimal()

ggplot(d.chains, aes(x=alpha, y=..density..)) + 
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) + 
  labs(title="Density of alpha", y="Density") + theme_minimal()

ggplot(d.chains, aes(x=iterations, y=beta, color=chains)) + geom_line(alpha=0.5) + 
  labs(title="Trace of beta", x="Iterations") + theme_minimal()

ggplot(d.chains, aes(x=beta, y=..density..)) + 
  geom_density(color="darkblue", fill="lightblue", alpha=0.5) + 
  labs(title="Density of beta", y="Density") + theme_minimal()
```

```{r, fig.show='hold', out.width="25%"}
## Rank plot of alpha
ggplot(d.chains[1:10000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=2, fill=2, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain1)") + theme_minimal()

ggplot(d.chains[10001:20000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=3, fill=3, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain2)") + theme_minimal()

ggplot(d.chains[20001:30000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=4, fill=4, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain3)") + theme_minimal()

ggplot(d.chains[30001:40000, ], aes(x=alphaRanks)) + 
  geom_histogram(bins=30, color=5, fill=5, alpha=0.5) + 
  labs(title="Rank plot of alpha (chain4)") + theme_minimal()

## Rank plot of beta
ggplot(d.chains[1:10000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=2, fill=2, alpha=0.5) + 
  labs(title="Rank plot of beta (chain1)") + theme_minimal()

ggplot(d.chains[10001:20000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=3, fill=3, alpha=0.5) + 
  labs(title="Rank plot of beta (chain2)") + theme_minimal()

ggplot(d.chains[20001:30000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=4, fill=4, alpha=0.5) + 
  labs(title="Rank plot of beta (chain3)") + theme_minimal()

ggplot(d.chains[30001:40000, ], aes(x=betaRanks)) + 
  geom_histogram(bins=30, color=5, fill=5, alpha=0.5) + 
  labs(title="Rank plot of beta (chain4)") + theme_minimal()
```

```{r, fig.show="hold", out.width="80%", fig.cap="Rank Plot.", fig.align='center'}
mcmc_rank_hist(fit2.jags.coda)
```


Compared to the rank plots obtained with the original model, the rank plots for $\beta$ in the new model do better follow a normal distribution. The rank plots of $\alpha$ are comparable. 


```{r}
## Heidelberger & Welch (Convergence to stationarity)
heidel.diag(fit2.jags.coda)
```

The p-values of the Heidelberg diagnostics have slightly changed. One test is now even failed (alpha 3).




```{r}
## Raftery & Lewis (Convergence to ergodic average)
raftery.diag(fit2.jags.coda)
```

The values for the dependence factor $I$ have decreased. They are all still below 5. So we do not obtain any strong autocorrelation.


```{r}
## Geweke (Convergence to stationarity)
geweke.diag(fit2.jags.coda)
```
The obtained absolute values for $\alpha$ and $\beta$ are all below 2, which indicates convergence to the target distribution. The plot shows that for the third and fourth chain some z-scores lie above and one below a z-score of 2.0. 


```{r, fig.show="hold", out.width="50%", fig.cap="Geweke Plot."}
geweke.plot(fit2.jags.coda)
```

```{r}
## Gelman and Rubin's convergence diagnostic
gelman.diag(fit2.jags.coda, autoburnin=TRUE)
```

The Gelman and Rubin diagnostic output is equivalent for this second adapted model to the one for the original model. 


```{r, fig.align='center', fig.dim="80%", fig.cap="Gelman plot."}
gelman.plot(fit2.jags.coda, autoburnin=TRUE)
```

```{r}
stable.GR(fit2.jags.coda)
```

The univariate potential scale reduction factors for both alpha and beta are both close to 1. Compared to the original model the effective sample size has been increased from 16766.74 to 29704.35.

\pagebreak
#  Exercise 6 (ESS)
Run the code from the previous exercise with mice data with only one chain monitoring *beta* under the following two conditions:

\begin{itemize}
\item[1.] \textbf{After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 1000 observations in one chain with thinning set to 1.}
\end{itemize}


```{r}
## Initialize starting points (let JAGS initialize) and set seed 
inits3.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159))

## Compile JAGS model
model3.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  n.chains = 1,
  inits = inits3.jags, 
  n.adapt = 1000
)

## Burn-in (increase burn-in iterations)
update(model3.jags, n.iter = 4000)

## Posterior Sampling
fit3.jags.coda <- coda.samples(
  model = model3.jags, 
  variable.names = c("beta"), #monitoring only beta
  n.iter = 1000,
  thin = 1
)
```

\begin{itemize}
\item[2.] \textbf{After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 10000 observations in one chain with thinning set to 10.}
\end{itemize}

```{r}
## Initialize starting points (let JAGS initialize) and set seed 
inits4.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159))

## Compile JAGS model
model4.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  n.chains = 1,
  inits = inits4.jags, 
  n.adapt = 1000
)

## Burn-in (increase burn-in iterations)
update(model4.jags, n.iter = 4000)

## Posterior Sampling
fit4.jags.coda <- coda.samples(
  model = model4.jags, 
  variable.names = c("beta"), #monitoring only beta
  n.iter = 10000,
  thin = 10
)
```

\begin{itemize}
\item[(a)] \textbf{For which of the above conditions the ESS estimates will be larger and why?}
\end{itemize}

```{r}
# Function to test if a vector is monoton decreasing,
# a boolean value is returned
monotone <- function(vec){
  a <- TRUE
  if(length(vec) == 1){
    return(a)
  }
  for(i in 2:length(vec)){
    if(vec[i] > vec[i-1]){
      a <- FALSE
      break;
    }
  }
  return(a)
}

# Function to find the lag to stop the ESS
# calculation compare: 
#
#      Geyer (1992),
#     "Practical Markov Chain Monte Carlo".
#      Statistical Science, 7: 473- 511
#
# Gamma_i = gamma(2*i) + gamma(2*i + 1)
#
# m is the greatest integer at which Gamma_i > 0
# and Gamma_i is monotone for i = 1, ..., m.
# Thereby gamma(i) is the sample autocorrelation 
# at lag i. 
#
# Parameter:
# vec - sample vector (mcmc object)
#
# Output
# m <- greatest integer where both criteria are
#     fulfilled
geyer <- function(vec){
  g <- c()
  res <- 1
   for(i in 1:(length(vec)/2 - 1)){
    g <- c(g,
           autocorr(vec, lags = i) + autocorr(vec, lags = i + 1)
           )
    if(monotone(g) == FALSE || g[i] < 0){
      break
    }
  }
  if(i==1){
    res <- 1
  }
  else{
    res <- i-1
  }
  return(res)
}
# Function to calculate the effective sample
# size for one MCMC chain.
#
# Parameter:
# mcmc - mcmc object 
# M - number of sampled values
#
# Output:
# effective sample size
ess <- function(mcmc, M){
  m <- geyer(mcmc)
  y <- M / (1 + 2 * sum(autocorr(mcmc, lag = 1:(2*m +1)) ) )
  return(y)
}
```


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{ESS}=N_{eff}&=\frac{M}{1+2\sum_{k\geq 1}^{\infty}\text{ACF}(k)}\\ -->
<!-- \text{where }& ACF(k)=\frac{Cov(\boldsymbol{x_{t},x_{t+k}})}{Var(\boldsymbol{x_{t}})}=\frac{\frac{1}{N}\sum_{t=1}^{N-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\frac{1}{N}\sum_{t=1}^{N}(x_{t}-\bar{x})^2} -->
<!-- \end{aligned} -->
<!-- $$ -->

$$
\text{ESS}=N_\text{eff}=\frac{M}{1+2\sum_{k\geq 1}^{\infty}\text{ACF}(k)}
$$
where $\text{ACF}(k)=\text{corr}(\theta_t^*,\theta_{t+k}^*)$


For practical computation, the infinite sum in the definition of ESS may be stopped earlier. Here the stopping is defined by the criteria of Geyer 1992.

We have in the second model a thinning parameter of 10 and thus expect the autocorrelation in the MCMC-sample to have less correlation as we pick only the 10th entry each time and put it into our MCMC-sample. Thus the sum will be smaller for the less correlated sample (`fit4.jags.coda`). $M$ is in both models the same with 1000. Therefore we expect the $ESS$ to be larger in the second model!


\begin{itemize}
\item[(b)] \textbf{To check your answer: Apply both the 05ess.R code and the function effectiveSize from the coda package. Compare the ESS estimates with those obtained with the n.eff function from package stableGR (Vats and Knudson, 2021). Please report your findings.}
\end{itemize}

```{r}
library(stableGR)
#ESS from the script

ess_script1 <- ess(mcmc=as.mcmc(fit3.jags.coda), M=length(as.mcmc(fit3.jags.coda)) )
ess_script2 <- ess(mcmc=as.mcmc(fit4.jags.coda), M=length(as.mcmc(fit4.jags.coda)) )

#ESS from the stableGR package
ess_function1 <- stableGR::n.eff(as.mcmc(fit3.jags.coda))
ess_function2 <- stableGR::n.eff(as.mcmc(fit4.jags.coda))

results <- data.frame(
              "method" = c("ESS-Script", "ESS-Script", "stableGR", "stableGR"),
              "thinning" = c(1,10,1,10),
              "n_eff" = c(ess_script1,ess_script2,ess_function1$n.eff,ess_function2$n.eff)
              )
```


```{r, echo=F}
knitr::kable(results, align="c", caption="Effective sample size with different thinning and functions.")
```

As expected, the effective sample size is with both methods higher in the model with thinning = 10 in comparison with thinning = 1.




