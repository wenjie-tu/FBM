---
title: "Worksheet 5"
author: 
  - Wenje Tu
  - Lea Bührer
  - Jerome Sepin
  - Zhixuan Li
  - Elia-Leonid Mastropietro
  - Jonas Raphael Füglistaler
date: "Spring Semester 2022"
output: pdf_document
# bibliography: biblio.bib
# nocite: '@*'
subtitle: Foundations of Bayesian Methodology
papersize: a4
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang="us_en")
rm(list=ls())
```


## Exercise 3 - Normal example in JAGS


## Exercise 4 - Logistic regression in JAGS


## Exercise 5 - CODA for logistic regression in JAGS

```{r}
x <- c(0.0028, 0.0028, 0.0056, 0.0112, 0.0225, 0.0450)
# the centered covariate values (centered dose) from the Mice data from Collett
x_centered <- x - mean(x)
# number of mice deaths
# y <- c(35, 21, 9, 6, 1)
y <- c(26, 9, 21, 9, 6, 1)
# total number of mice
# n <- c(40, 40, 40, 40, 40)
n <- c(28, 12, 40, 40, 40, 40)
```

```{r mice-table, results='asis'}
d.mice <- data.frame(
  x, y, n, x_centered, y/n
)
colnames(d.mice) <- c("$x$", "$y$", "$n$", "centered $x$", "$p$")
knitr::kable(d.mice, align="c", caption="Mice data from Collett (2003)")
```

Logistic model:

$$
\text{logit}(p_i)=\ln\left(\frac{p_i}{1-p_i} \right)=\alpha+\beta x_i
$$

$$
p_i=\frac{\exp(\alpha+\beta x_i)}{1+\exp(\alpha+\beta x_i)}
$$

```{r}
## Disaggregate the data
d.mice1 <- data.frame(Response=rep(c(1, 0), c(sum(y), sum(n)-sum(y))), 
                      CenteredDose=c(rep(round(x_centered, 5), y), 
                                     rep(round(x_centered, 5), n-y)))

knitr::kable(table(d.mice1))
```

```{r, fig.align='center', out.width="75%"}
mosaicplot(CenteredDose ~ Response, data=d.mice1, color=2:3, 
           main="Mosaic Plot", xlab="Centered Dosage")
```


```{r}
modelString <- "model{
  for (i in 1:length(y)) {
    y[i] ~ dbern(p[i])
    p[i] <- ilogit(alpha + beta * x[i])
  }
  
  alpha ~ dnorm(0, 1.0E-04)
  beta ~ dnorm(0, 1.0E-04)
}"

writeLines(modelString, con="LogitModel.txt")
```

```{r, message=F, warning=F}
library(rjags)
library(coda)
```


```{r}
## Generate data list for JAGS
dat.jags <- with(d.mice1, list(y=Response, x=CenteredDose))

## Initialize starting points (let JAGS initialize) and set seed 
inits.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159),
                   list(.RNG.name="base::Marsaglia-Multicarry", .RNG.seed=159314),
                   list(.RNG.name="base::Super-Duper", .RNG.seed=413159),
                   list(.RNG.name="base::Mersenne-Twister", .RNG.seed=143915))

## Compile JAGS model
model1.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  inits = inits.jags, 
  n.chains = 4,
  n.adapt = 4000
)
```

```{r}
## Burn-in
update(model1.jags, n.iter = 4000)

## Sampling
fit1.jags.coda <- coda.samples(
  model = model1.jags, 
  variable.names = c("alpha", "beta"), 
  n.iter = 10000,
  thin = 1
)
```

```{r}
summary(fit1.jags.coda)
```


```{r, fig.show="hold", out.width="80%", fig.cap="Trace and density plot.", fig.align='center'}
plot(fit1.jags.coda)
```

```{r}
m.fit1.jags.coda <-as.matrix(fit1.jags.coda)
d.summary <- t(rbind(
  colMeans(m.fit1.jags.coda), 
  apply(m.fit1.jags.coda, 2, function(x) sd(x)), 
  apply(m.fit1.jags.coda, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
))

colnames(d.summary) <- c("Mean", "SD", "2.5%", "Median", "97.5%")
knitr::kable(d.summary, align="c", caption="Summary statistics")
```

### 5.1 Rank plots



```{r, warning=FALSE, message=FALSE}
library(bayesplot)
library(stableGR)
```

```{r rankplot, fig.show="hold", out.width="80%", fig.cap="Rank Plot.", fig.align='center'}
mcmc_rank_hist(fit1.jags.coda)
```
The figure illustrates the histograms of the ranked posterior drawn, ranked over all chains for $\alpha$ and $\beta$. Whereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.
See Vehtari et al. (2019) for details.

It can be seen in the figure that the rank histograms nearly follow a normal distribution.



### 5.2 Convergence diagnostics

#### (a) Heidelberger \& Welch (Convergence to stationarity)

```{r}
heidel.diag(fit1.jags.coda)
```

The output from `heidel.diag` shows the summary results for the four generated chains.
The daignostic on one hand checks if the length of the sample it long enough and on the other hand checks if the means are estimated from a converged chain. 

The stationarity test uses the Cramer-von-Mises statistic to test the null hypothesis that the sampled values come from a stationary distribution. The half-width test calculates a 95% confidence interval for the mean, using the portion of the chain which passed the stationarity test. 

We see that both tests are passed for the four chains and all $p$-values are larger than 0.05 = $\alpha$. We conclude that the sampled values come from a stationary distribution and the length of the sample is considered long enough to estimate the mean with sufficient accuracy.


####  (b) Raftery \& Lewis (Convergence to ergodic average)

```{r}
raftery.diag(fit1.jags.coda)
```

The output from `raftery.diag` shows the summary results for the four generated chains. `raftery.diag` is a run length control diagnostic based on a criterion of accuracy of estimation of the quantile $q$. The dependence factor $I$ ($I=\frac{M+N}{N_\text{min}}$), indicates to which extent autocorrelation inflates the required sample size. $I>5$ indicates strong autocorrelation which may be due to a poor choice of starting value, high posterior correlations or "stickiness" of the MCMC algorithm.  We see that the dependence factors for the four chains are all smaller than 5 and hence no strong autocorrelation exists. 


#### (c) Geweke (Convergence to stationarity)

```{r}
geweke.diag(fit1.jags.coda)
```

The output from `geweke.diag` shows the summary results for the four generated chains. `geweke.diag` is a convergence diagnostic for Markov chains based on a test for equality of the means of the first and last part of a Markov chain (by default the first 10% and the last 50%). If the samples are drawn from the stationary distribution of the chain, the two means are equal and Geweke's statistic has an asymptotically standard normal distribution. The test statistic is a standard Z-score.

The idea behind the Geweke's diagnostic is that in a long enough chain whose trace plots suggest convergence to the target distribution, we assume the second half of the chain has converged to the target distribution and we test if the first 10% can be treated as burn-in. So we mimic the simple two-sample test of means: if the mean of the first 10% is not significantly different from the last 50%, then we conclude the target distribution converged somewhere in the first 10% of the chain. So we'll use this 10% as burn-in. 

We see that for the four chains the absolute values of Z-scores for variables alpha and beta are smaller than 2, which indicates that the equilibrium may have been reached. 

```{r,  fig.show="hold", out.width="50%", fig.cap="Geweke Plot."}
geweke.plot(fit1.jags.coda)
```

If `geweke.diag` indicates that the first and last part of a sample from a Markov chain are not drawn from the same distribution, it may be useful to discard the first few iterations to see if the rest of the chain has "converged". `geweke.plot` shows what happens to Geweke's Z-score when successively larger numbers of iterations are discarded from the beginning of the chain. To preserve the asymptotic conditions required for Geweke's diagnostic, the plot never discards more than half the chain. For beta (chain1), alpha (chain2), and alpha (chain3), there exist several segments out of the 95% confidence bands ($\lvert Z \rvert > 2$). 
The plot shows that we mainly obtain z-score values below an absolute value of 2. So we can assume that the chain has converged. 


#### (d) Gelman and Rubin's convergence diagnostic

```{r}
gelman.diag(fit1.jags.coda, autoburnin=TRUE)
```

The idea of Gelman and Rubin's convergence diagnostic is to run multiple chains from widely dispersed starting values and perform an Analysis of Variance to see if the between-chain variability ($B$) is large in relation to the average variability within ($W+B$) the pooled chain. 

$$
\sqrt{\hat{R}}\approx
\sqrt{\frac{W+B}{W}}
$$

$\hat{R}$ is so-called "potential scale reduction factor" and it is calculated for each variable. We see that the "Potential scale reduction factors" for both alpha and beta are close to 1, which indicates that the between-chain variability ($B$) is close to 0. In other words, the separate four chains have mixed quite well.

```{r, fig.align='center', fig.dim="70%", fig.cap="Gelman plot."}
gelman.plot(fit1.jags.coda, autoburnin=TRUE)
```

The `gelman.plot` shows that the evolution of Gelman and Rubin's shrink factor as the number of iterations increases. We see that the shrink factors for both variables quickly decrease to 1 after 10000 iterations and the variability of shrink factors becomes more stable as the number of iterations keeps increasing.

####  (e) Gelman-Rubin diagnostic using stable variance estimators

```{r}
stable.GR(fit1.jags.coda)
```

`stable.GR` extends Gelman-Rubin diagnostic using stable variance estimators. We see that the univariate potential scale reduction factors for both alpha and beta are calculated and they are close to 1. Multivariate `psrf` is also calculated by taking into account the interdependence of the chain's components. The PSRFs decrease to 1 as the chain length increases. When the PSRF becomes sufficiently close to 1, the sample collected by the Markov chain has converged to the target distribution. Means of variables (alpha and beta) and the effective sample size are also reported in the output. 

#### Conclusions:

- Heidel: We conclude that the sampled values come from a stationary distribution and the length of the sample is considered long enough to estimate the mean with  sufficient accuracy.

- Raftery \& Lewis: The dependence factors for the four chains are all smaller than 5 and hence no strong autocorrelation exists. 

- Geweke: We assume that the equilibrium has been reached. We achieve convergence towards the target distribution.

- Gelman \& Rubin: The between-chain variability is close to 0. The separate four chains have mixed quite well. 

- Gelman \& Rubin for stable variance: The chains have converged to the target distribution.


All in all, it can be concluded that no issues arose during the diagnostics. 



### 5.3 Re-run the MCMC simulation

In the following steps the MCMC simulation is adapted to the findings from subtask 5.2.
We keep the same model as before but we change n.iter from 10'000 to 30'000 and n.thin from 1 to 3. The number of burn-in iterations (4000) is kept. **WHY do we change to this values???? the diagnostics do not really get better... (excluding the rank_hist)**


```{r}
## Compile JAGS model
model2.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  inits = inits.jags, 
  n.chains = 4,
  n.adapt = 4000
)
```

```{r}
## Burn-in (increase burn-in iterations)
update(model2.jags, n.iter = 4000)

## Initialize starting points (let JAGS initialize) and set seed 
inits.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159),
                   list(.RNG.name="base::Marsaglia-Multicarry", .RNG.seed=159314),
                   list(.RNG.name="base::Super-Duper", .RNG.seed=413159),
                   list(.RNG.name="base::Mersenne-Twister", .RNG.seed=143915))

## Sampling
fit2.jags.coda <- coda.samples(
  model = model2.jags, 
  variable.names = c("alpha", "beta"), 
  n.iter = 30000,
  thin = 3
)
```

```{r, fig.show="hold", out.width="80%", fig.cap="Trace and density plot.", fig.align='center'}
summary(fit2.jags.coda)
plot(fit2.jags.coda)
```



```{r, fig.show="hold", out.width="80%", fig.cap="Rank Plot.", fig.align='center'}
mcmc_rank_hist(fit2.jags.coda)
```


Compared to the rank plots obtained with the original model, the rank plots for $\beta$ in the new model do better follow a normal distribution. The rank plots of $\alpha$ are comparable. 


```{r}
## Heidelberger & Welch (Convergence to stationarity)
heidel.diag(fit2.jags.coda)
```

The p-values of the Heidelberg diagnostics have slightly changed. One test is now even failed (alpha 3).




```{r}
## Raftery & Lewis (Convergence to ergodic average)
raftery.diag(fit2.jags.coda)
```

The values for the dependence factor $I$ have decreased. They are all still below 5. So we do not obtain any strong autocorrelation.


```{r}
## Geweke (Convergence to stationarity)
geweke.diag(fit2.jags.coda)
```
The obtained absolute values for $\alpha$ and $\beta$ are all below 2, which indicates convergence to the target distribution. The plot shows that for the third and fourth chain some z-scores lie above and one below a z-score of 2.0. 


```{r, fig.show="hold", out.width="50%", fig.cap="Geweke Plot."}
geweke.plot(fit2.jags.coda)
```

```{r}
## Gelman and Rubin's convergence diagnostic
gelman.diag(fit2.jags.coda, autoburnin=TRUE)
```

The Gelman and Rubin diagnostic output is equivalent for this second adapted model to the one for the original model. 


```{r, fig.align='center', fig.dim="80%", fig.cap="Gelman plot."}
gelman.plot(fit2.jags.coda, autoburnin=TRUE)
```

```{r}
stable.GR(fit2.jags.coda)
```

The univariate potential scale reduction factors for both alpha and beta are both close to 1. Compared to the original model the effective sample size has been increased from 16766.74 to 29704.35.

\pagebreak
##  Exercise 6 (ESS)
Run the code from the previous exercise with mice data with only one chain monitoring *beta* under the following two conditions:

\begin{itemize}
\item[1.] \textbf{After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 1000 observations in one chain with thinning set to 1.}
\end{itemize}


```{r}
## Initialize starting points (let JAGS initialize) and set seed 
inits3.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159))

## Compile JAGS model
model3.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  n.chains = 1,
  inits = inits3.jags, 
  n.adapt = 1000
)

## Burn-in (increase burn-in iterations)
update(model3.jags, n.iter = 4000)

## Posterior Sampling
fit3.jags.coda <- coda.samples(
  model = model3.jags, 
  variable.names = c("beta"), #monitoring only beta
  n.iter = 1000,
  thin = 1
)
```

\begin{itemize}
\item[2.] \textbf{After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 10000 observations in one chain with thinning set to 10.}
\end{itemize}

```{r}
## Initialize starting points (let JAGS initialize) and set seed 
inits4.jags <- list(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=314159))

## Compile JAGS model
model4.jags <- jags.model(
  file = "LogitModel.txt", 
  data = dat.jags,
  n.chains = 1,
  inits = inits4.jags, 
  n.adapt = 1000
)

## Burn-in (increase burn-in iterations)
update(model4.jags, n.iter = 4000)

## Posterior Sampling
fit4.jags.coda <- coda.samples(
  model = model4.jags, 
  variable.names = c("beta"), #monitoring only beta
  n.iter = 10000,
  thin = 10
)
```

\begin{itemize}
\item[(a)] \textbf{For which of the above conditions the ESS estimates will be larger and why?}
\end{itemize}

```{r}
# Function to test if a vector is monoton decreasing,
# a boolean value is returned
monotone <- function(vec){
  a <- TRUE
  if(length(vec) == 1){
    return(a)
  }
  for(i in 2:length(vec)){
    if(vec[i] > vec[i-1]){
      a <- FALSE
      break;
    }
  }
  return(a)
}

# Function to find the lag to stop the ESS
# calculation compare: 
#
#      Geyer (1992),
#     "Practical Markov Chain Monte Carlo".
#      Statistical Science, 7: 473- 511
#
# Gamma_i = gamma(2*i) + gamma(2*i + 1)
#
# m is the greatest integer at which Gamma_i > 0
# and Gamma_i is monotone for i = 1, ..., m.
# Thereby gamma(i) is the sample autocorrelation 
# at lag i. 
#
# Parameter:
# vec - sample vector (mcmc object)
#
# Output
# m <- greatest integer where both criteria are
#     fulfilled
geyer <- function(vec){
  g <- c()
  res <- 1
   for(i in 1:(length(vec)/2 - 1)){
    g <- c(g,
           autocorr(vec, lags = i) + autocorr(vec, lags = i + 1)
           )
    if(monotone(g) == FALSE || g[i] < 0){
      break
    }
  }
  if(i==1){
    res <- 1
  }
  else{
    res <- i-1
  }
  return(res)
}
# Function to calculate the effective sample
# size for one MCMC chain.
#
# Parameter:
# mcmc - mcmc object 
# M - number of sampled values
#
# Output:
# effective sample size
ess <- function(mcmc, M){
  m <- geyer(mcmc)
  y <- M / (1 + 2 * sum(autocorr(mcmc, lag = 1:(2*m +1)) ) )
  return(y)
}
```


$$
\begin{aligned}
ESS=N_{eff}&=\frac{M}{1+2\sum_{k\geq 1}^{\infty}ACF(k)}\\
\text{where }& ACF(k)=\frac{Cov(\boldsymbol{x_{t},x_{t+k}})}{Var(\boldsymbol{x_{t}})}=\frac{\frac{1}{N}\sum_{t=1}^{N-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\frac{1}{N}\sum_{t=1}^{N}(x_{t}-\bar{x})^2}
\end{aligned}
$$

For practical computation, the infinite sum in the definition of ESS may be stopped earlier. Here the stopping is defined by the criteria of Geyer 1992.

We have in the second model a thinning parameter of 10 and thus expect the autocorrelation in the MCMC-sample to have less correlation as we pick only the 10th entry each time and put it into our MCMC-sample. Thus the sum will be smaller for the less correlated sample (`fit4.jags.coda`). $M$ is in both models the same with 1000. Therefore we expect the $ESS$ to be larger in the second model!


\begin{itemize}
\item[(b)] \textbf{To check your answer: Apply both the 05ess.R code and the function effectiveSize from the coda package. Compare the ESS estimates with those obtained with the n.eff function from package stableGR (Vats and Knudson, 2021). Please report your findings.}
\end{itemize}

```{r}
library(stableGR)
#ESS from the script
ess_script1 <- ess(mcmc = as.mcmc(fit3.jags.coda), M= 1000)
ess_script2 <- ess(mcmc = as.mcmc(fit4.jags.coda), M= 1000)

#ESS from the stableGR package
ess_function1 <- stableGR::n.eff(as.mcmc(fit3.jags.coda))
ess_function2 <- stableGR::n.eff(as.mcmc(fit4.jags.coda))

results <- data.frame(
              "n_eff" = c(ess_script1,ess_script2,ess_function1$n.eff,ess_function2$n.eff),
              "thining" = c(1,10,1,10),
              "method"  = c("ESS-Script", "ESS-Script", "stableGR", "stableGR")
              )
```


```{r,echo=F}
knitr::kable(results,align = "c", caption = "Effective sample size with different thinning and functions.")
```

As expected, the effective sample size is with both methods higher in the model with thinning = 10 in comparison with thinning = 1.




