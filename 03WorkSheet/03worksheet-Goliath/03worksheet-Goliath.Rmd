---
title: "Worksheet 3"
author: 
  - Wenje Tu
  - Lea Bührer
  - Jerome Sepin
  - Zhixuan Li
  - Elia-Leonid Mastropietro
  - Jonas Raphael Füglistaler
date: "Spring Semester 2022"
output: pdf_document
# bibliography: biblio.bib
# nocite: '@*'
subtitle: Foundations of Bayesian Methodology
papersize: a4
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Exercise 3 (Conjugate Bayes: analytical derivation)

Assumptions: 
$$
\begin{aligned}
y_1,\cdots,y_n\mid m  & \overset{i.i.d}{\sim} \mathcal{N}(m,\kappa^{-1}) \\
m & \: \sim \:\mathcal{N}(\mu,\lambda^{-1})
\end{aligned}
$$

**3 (a)**

The prior predictive distribution of one future observation $y$ assuming that no observations have been collected yet:

$$
\begin{aligned}
f(y)
&=\int_{-\infty}^{\infty}f(y\mid m)f(m)\text{d}m \\
&=\int_{-\infty}^{\infty}\sqrt{\frac{\kappa}{2\pi}}\exp\left(-\frac{\kappa}{2}(y-m)^2 \right)
\sqrt{\frac{\lambda}{2\pi}}\exp\left(-\frac{\lambda}{2}(m-\mu)^2 \right)\text{d}m \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\int_{-\infty}^{\infty}
\exp\left(-\frac{1}{2}\left(\kappa(y^2-2ym+m^2)+\lambda(m^2-2m\mu+\mu^2) \right) \right)\text{d}m \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\int_{-\infty}^{\infty}
\exp\left(-\frac{1}{2}\left((\kappa+\lambda)\left(m-\frac{\kappa y+\lambda\mu}{\kappa+\lambda} \right)^2
-\frac{(\kappa y+\lambda\mu)^2}{\kappa+\lambda}+\kappa y^2+\lambda \mu^2
\right) \right)\text{d}m \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\int_{-\infty}^{\infty}
\exp\left(-\frac{1}{2}\left((\kappa+\lambda)\left(m-\frac{\kappa y+\lambda\mu}{\kappa+\lambda} \right)^2
+\frac{\kappa\lambda(y-\mu)^2}{\kappa+\lambda}
\right) \right)\text{d}m \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\exp\left(-\frac{\kappa\lambda(y-\mu)^2}{2(\kappa+\lambda)}\right)
\int_{-\infty}^{\infty}
\exp\left(-\frac{\kappa+\lambda}{2}\left(m-\frac{\kappa y+\lambda\mu}{\kappa+\lambda} \right)^2
\right)\text{d}m \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\sqrt{\frac{2\pi}{\kappa+\lambda}}
\exp\left(-\frac{\kappa\lambda(y-\mu)^2}{2(\kappa+\lambda)}\right)
\underbrace{
\int_{-\infty}^{\infty}\sqrt{\frac{\kappa+\lambda}{2\pi}}
\exp\left(-\frac{\kappa+\lambda}{2}\left(m-\frac{\kappa y+\lambda\mu}{\kappa+\lambda} \right)^2
\right)\text{d}m}_{=1} \\
&=\frac{\sqrt{\kappa\lambda}}{2\pi}\sqrt{\frac{2\pi}{\kappa+\lambda}}
\exp\left(-\frac{\kappa\lambda(y-\mu)^2}{2(\kappa+\lambda)}\right) \\
&=\sqrt{\frac{1}{2\pi\left(\frac{1}{\lambda}+\frac{1}{\kappa} \right)}}
\exp\left(-\frac{(y-\mu)^2}{2\left(\frac{1}{\lambda}+\frac{1}{\kappa} \right)}\right)
\end{aligned}
$$

The prior predictive distribution of one future observation $y$ is
$$
\mathcal{N}(\mu, \lambda^{-1}+\kappa^{-1})
$$

**3 (b)**

The posterior predictive distribution of one future observation $y_{n+1}$ given that $y_1,\cdots,y_n$ have been observed:

$$
\begin{aligned}
f(y_{n+1}\mid y_1,\cdots,y_n)
&=\int_{-\infty}^{\infty}f(y_{n+1},m\mid y_1,\cdots,y_n)\text{d}m \\
&=\int_{-\infty}^{\infty}f(y_{n+1}\mid m, y_1,\cdots,y_n)f(m\mid y_1,\cdots,y_n)\text{d}m \\
&=\int_{-\infty}^{\infty}f(y_{n+1}\mid m)f(m\mid y_1,\cdots,y_n)\text{d}m \\
\end{aligned}
$$

$$
f(m\mid y_1,\cdots,y_n)=\sqrt{\frac{n\kappa+\lambda}{2\pi}}\exp\left(-\frac{n\kappa+\lambda}{2}\left(m-\frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \right)^2  \right)
$$

Denote:

$$
\begin{aligned}
  \mu_\text{post} &= \frac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda} \\
  \lambda_\text{post} &= n\kappa+\lambda
\end{aligned}
$$

$$
f(m\mid y_1,\cdots,y_n)=\sqrt{\frac{\lambda_\text{post}}{2\pi}}\exp\left(-\frac{\lambda_\text{post}}{2}\left(m-\mu_\text{post} \right)^2  \right)
$$

$$
f(y_{n+1}\mid y_1,\cdots,y_n)
=\int_{-\infty}^{\infty}\sqrt{\frac{\kappa}{2\pi}}\exp\left(-\frac{\kappa}{2}(y_{n+1}-m)^2 \right)
\underbrace{\sqrt{\frac{\lambda_\text{post}}{2\pi}}\exp\left(-\frac{\lambda_\text{post}}{2}\left(m-\mu_\text{post} \right)^2  \right)}_{\text{posterior distribution}}\text{dm} 
$$

Repeating the same derivation steps as for the prior predictive distribution, we obtain the posterior predictive distribution:

$$
y_{n+1}\mid y_1,\cdots,y_n\sim\mathcal{N}(\mu_\text{post}, \lambda_\text{post}^{-1}+\kappa^{-1})
$$

$~$

### Exercise 4 (Conjugate Bayesian analysis in practice)

**4 (a)**

Plot the prior predictive distribution for one observation y and compute its expectation
and standard deviation. Estimate P[y > 200] for one future observation of Height.

Prior predictive distribution:
$$
y\sim \mathcal{N}(\mu, \lambda^{-1}+\kappa^{-1})=\mathcal{N}(161, 970)
$$

```{r}
height <- c(166,168,168,177,160,170,172,159,175,164,175,167,164)
n      <- length(height) # sample size
y.bar  <- mean(height)   # sample mean
kappa  <- 1/900          # precision for data
mu     <- 161            # prior mean
lambda <- 1/70           # prior precision

## Compute posterior mean and precision
mu.post     <- (kappa*n*y.bar + lambda*mu) / (n*kappa + lambda)
lambda.post <- n*kappa + lambda
```

The expectation of the prior predictive distribution for one observation $y$ is `r format(mu, digits=4)` and the standard deviation is $\sqrt{\lambda^{-1}+\kappa^{-1}} =$  `r format(sqrt((1/lambda) + (1/kappa)), digits = 4)`.

Prior predictive distribution:
```{r, fig.show="hold", out.width="75%", fig.align='center'}
curve(dnorm(x, mean=mu, sd=sqrt(1/kappa)), ylim=c(0, 0.015), xlim=c(71, 251), 
      col=2, lwd=2, xlab="Height", ylab="Density", main="Prior predictive distribution")
```


```{r}
## P[y > 200] for one future observation of Height
pnorm(200, mean=mu, sd=sqrt(1/lambda + 1/kappa), lower.tail=F)
```
$P[y>200]$ for one future observation of `Height` is 0.105. So it can be concluded that we expect around 10% of future observation to be larger than 200 cm. 


**4 (b)**

```{r}
## Compute posterior mean and precision
mu.post     <- (kappa*n*y.bar + lambda*mu) / (n*kappa + lambda)
lambda.post <- n*kappa + lambda

cat(sprintf("Posterior mean is %.4f \nPosterior inversed precision is %.4f", 
            mu.post, 1/lambda.post))
```

Posterior distribution:
$$
m\mid y_1,\cdots,y_n
\sim\mathcal{N}\left(\mu_\text{post}, \lambda_\text{post}^{-1}\right)
\equiv\mathcal{N}(164.558, 34.8066)
$$

Posterior predictive distribution:
$$
y_{n+1}\mid y_1,\cdots,y_n
\sim\mathcal{N}\left(\mu_\text{post}, \lambda_\text{post}^{-1}+\kappa^{-1}\right)
\equiv\mathcal{N}(164.558, 934.8066)
$$
where $\mu_\text{post}=\dfrac{\kappa n\bar{y}+\lambda\mu}{n\kappa+\lambda}$ and $\lambda_\text{post}=n\kappa+\lambda$.

The expectation of the posterior predictive distribution for one observation $y_{n+1}$ is `r format(mu.post, digits = 4)` and the standard deviation is $\sqrt{\lambda_\text{post}^{-1}+\kappa^{-1}} =$  `r format(sqrt((1/lambda.post) + (1/kappa)), digits = 4)`.

```{r, fig.show="hold", out.width="75%", fig.align='center'}
curve(dnorm(x, mean=mu.post, sd=sqrt(1/lambda.post + 1/kappa)), ylim=c(0, 0.015), 
      xlim=c(61, 261), col=4, lwd=2, xlab="Height", ylab="Density", 
      main="Posterior predictive distribution")
```


Estimate for $P[y_{n+1}> 200\mid y_1,...,y_n]$ for one future observation $y_{n+1}$:
```{r}
pnorm(200, mean=mu.post, sd=sqrt(1/lambda.post+1/kappa), lower.tail=F)
```
$P[y_{n+1}> 200\mid y_1,...,y_n]$ for one future observation $y_{n+1}$ of `Height` is 0.123.

<!-- As a result we obtain a probability of 12.3% that a future observation of height will be larger than 200 cm. -->


**4 (c)**

Comparison between posterior, prior predictive, and posterior distributions

```{r, fig.show="hold", out.width="50%"}
curve(dnorm(x, mean=164.558, sd=sqrt(34.80663)), ylim=c(0, 0.07), 
      xlim=c(111, 211), col=2, lwd=2, xlab="Height", ylab="Density", 
      main="Posterior, prior predictive and posterior predictive distributions")
curve(dnorm(x, mean=161, sd=sqrt(970)), col=3, lwd=2, add=TRUE)
curve(dnorm(x, mean=164.558, sd=sqrt(934.80663)), col=4, lwd=2, add=TRUE)
legend("topleft", col=2:4, lwd=2, 
       legend=c("posterior", "prior predictive", "posterior predcitive"))

curve(dnorm(x, mean=161, sd=sqrt(970)), ylim=c(0, 0.015), xlim=c(61, 261), col=3, lwd=2,
      xlab="Height", ylab="Density", main="Prior and posterior predictive distributions")
curve(dnorm(x, mean=164.558, sd=sqrt(934.80663)), col=4, lwd=2, add=TRUE)
legend("topleft", legend=c("prior predictive", "posterior predictive"), col=3:4, lwd=2)
```

```{r, fig.show="hold", out.width="50%", include=FALSE}
curve(dnorm(x, mean=164.558, sd=sqrt(34.80663)), ylim=c(0, 0.07), xlim=c(111, 211), 
      col=2, lwd=2, xlab="Height", ylab="Density", 
      main="Posterior and prior distributions")
curve(dnorm(x, mean=161, sd=sqrt(70)), col=1, lwd=2, add=TRUE)
curve(dnorm(x, mean=161, sd=sqrt(970)), ylim=c(0, 0.015), xlim=c(61, 261), col=3, lwd=2, 
      add=T)
curve(dnorm(x, mean=164.558, sd=sqrt(934.80663)), col=4, lwd=2, add=TRUE)
legend("topleft", legend=c("prior", "posterior","prior predictive", "posterior predictive"), 
       col=c(1:4), lwd=2)

curve(dnorm(x, mean=161, sd=sqrt(970)), ylim=c(0, 0.015), xlim=c(61, 261), col=3, lwd=2, 
      xlab="Height", ylab="Density", main="Prior and posterior predictive distributions")
curve(dnorm(x, mean=164.558, sd=sqrt(934.80663)), col=4, lwd=2, add=TRUE)
legend("topleft", legend=c("prior predictive", "posterior predictive"), col=3:4, lwd=2)
```

```{r, include=FALSE}
df <- data.frame(
  Mean=c(mu, mu.post, mu, mu.post), 
  Variance=c(1/lambda, 1/lambda.post, 1/lambda+1/kappa, 1/lambda.post+1/kappa), 
  SD=c(sqrt(1/lambda),sqrt(1/lambda.post), sqrt(1/lambda+1/kappa), sqrt(1/lambda.post+1/kappa))
)
rownames(df) <- c("Prior","Posterior", "Prior predictive", "Posterior predictive")
knitr::kable(df, align="c", caption="Summary statistics")
```

```{r}
df <- data.frame(
Mean=c(mu.post, mu, mu.post),
Variance=c(1/lambda.post, 1/lambda+1/kappa, 1/lambda.post+1/kappa),
SD=c(sqrt(1/lambda.post), sqrt(1/lambda+1/kappa), sqrt(1/lambda.post+1/kappa))
)
rownames(df) <- c("Posterior", "Prior predictive", "Posterior predictive")
knitr::kable(df, align="c", caption="Summary statistics")
```


In this example, the prior predictive and posterior predictive distributions do not differ too much. However, the posterior distribution differs dramatically from the other two predictive distributions, and this is attributed to the difference in the variance of the distributions. 

The posterior distribution refers to the distribution of the parameter $m$ while the prior (posterior) predictive distribution refers to the distribution of one future observation of `Height`. The posterior distribution has a more narrow shape (i.e. smaller variance) and it implies that we are more certain about the $m$. Under extreme circumstances where $\lim\limits_{n\to\infty}(n\kappa+\lambda)^{-1}\approx0$, there is little variance in the posterior distribution, but it does not mean that there is no variance in posterior predictive distribution. Posterior predictive distribution ensures that we are not too optimistic about the distribution of future observations.

$~$

### Exercise 5 (The change-of-variables formula)

$$
X\sim\text{Gamma}(a,b)
$$

$$
f(x)=\frac{b^a}{\Gamma(a)}x^{a-1}\exp(-bx)
$$

$$
\begin{aligned}
P(Y\leq y)
&=P(g(X)\leq y) \\
&=P(X\leq g^{-1}(y)) \\
F_Y(y)
&=F_X(g^{-1}(y))
\end{aligned}
$$

By differentiating the CDFs on both sides w.r.t. $y$, we can get the PDF of $Y$.

$$
f_Y(y)=
\begin{cases}
f_X(g^{-1}(y))\cdot\dfrac{d}{dy}g^{-1}(y)  & g(\cdot)\:\text{is monotonically increasing} \\
-f_X(g^{-1}(y))\cdot\dfrac{d}{dy}g^{-1}(y) & g(\cdot)\:\text{is monotonically decreasing}
\end{cases}
$$

Therefore:
$$
f_Y(y)=f_X(g^{-1}(y))\cdot\left\lvert \frac{d}{dy}g^{-1}(y)\right\rvert
$$

$$
f(x)=\frac{b^a}{\Gamma(a)}x^{a-1}\exp(-bx)
$$

$$
Y=\frac{1}{X} \implies X=\frac{1}{Y}
$$
Thus $g^{-1}(y)=y^{-1}$ with the derivative $g^{-1}(y)\frac{d}{dy}=-y^{-2}$

$$
\begin{aligned}
f_Y(y)
&= f_X(g^{-1}(y))\cdot\left\lvert \frac{d}{dy}g^{-1}(y)\right\rvert \\
&= \frac{b^a}{\Gamma(a)}\left(\frac{1}{y}\right)^{a-1}\exp(-\frac{b}{y})
\cdot\left\lvert-\frac{1}{y^2} \right\rvert \\
&= \frac{b^a}{\Gamma(a)}\left(\frac{1}{y}\right)^{a+1}\exp(-\frac{b}{y})
\end{aligned}
$$

$$
Z=\sqrt{\frac{1}{X}} \implies X=\frac{1}{Z^2}
$$

Thus $g^{-1}(z)=z^{-2}$ with the derivative $g^{-1}(z)\frac{d}{dz}=-2z^{-3}$


$$
\begin{aligned}
f_Z(z)
&= f_X(g^{-1}(z))\cdot\left\lvert \frac{d}{dz}g^{-1}(z)\right\rvert \\
&= \frac{b^a}{\Gamma(a)}\left(\frac{1}{z^2}\right)^{a-1}\exp(-\frac{b}{z^2})
\cdot\left\lvert-\frac{2}{z^3} \right\rvert \\
&= \frac{b^a}{\Gamma(a)}2\left(\frac{1}{z}\right)^{2a+1}\exp(-\frac{b}{z^2})
\end{aligned}
$$

```{r}
## Define inverse-gamma distribution function
dinvgamma <- function(x, a, b) {
  return(
    (b^a)/gamma(a) * (1/x)^(a+1) * exp(-b/x)
  )
}

## Define square root inverse-gamma distribution function
dsqrtinvgamma <- function(x, a, b) {
  return(
    2 * (b^a)/gamma(a) * (1/x)^(2*a+1) * exp(-b/(x^2))
  )
}
```


```{r}
a <- 1.6
b <- 0.4
```

```{r, include=FALSE}
curve(dgamma(x, shape=a, rate=b), xlim=c(0, 1.5), ylim=c(0, 2.5), col=2, lwd=3, 
      xlab="X", ylab="Density", main="Density of X")
curve(dinvgamma(x, a, b), col=3, lwd=2, add=TRUE)
curve(dsqrtinvgamma(x, a, b), add=TRUE, col=4, lwd=2)
legend("topright", legend=c("G", "IG", "SIG"), col=2:4, lwd=2)
```

```{r, fig.show="hold", out.width="50%"}
curve(dgamma(x, shape=a, rate=b), xlim=c(0, 8), ylim=c(0, 0.2), col=2, lwd=3, 
      xlab="X", ylab="Density", main="Density of X")
legend("topleft", legend="G", col=2, lwd=2)

curve(dinvgamma(x, a, b), xlim=c(0, 0.5), col=3, lwd=2, 
      main="Densities of Y and Z", y="Density", xlab="Y or Z")
curve(dsqrtinvgamma(x, a, b), add=TRUE, col=4, lwd=2)
legend("topleft", legend=c("IG", "SIG"), col=c(3, 4), lwd=2)
```

By overlaying the densities of $Y$ and $Z$, we see that the shape of densities is nearly flat when $Y$ and $Z$ are very close to 0, which implies that it is very unlikely that a random draw from Inverse Gamma distribution or from Square root Inverse Gamma distribution would be very close to 0. Recall that a Gamma prior is used for the precision $\frac{1}{\sigma^2}$, an Inverse Gamma prior is used for the variance $\sigma^2$, and a Square root Inverse Gamma prior is used for the standard deviation $\sigma$. It also implies that the prior about the variance (or the standard deviation) **cannot** be infinitesimally small (i.e. very close to 0).


$~$

### Exercise 6 (Monte Carlo: transformations of random variables)

```{r}
## Set seed for reproducible results
set.seed(44566)

## Parameters for Gamma
a <- 1.6 # shape
b <- 0.4 # rate (inverse of scale)

## MC sample size
M <- 1000
```

```{r}
## Generate a MC sample of size 1000 from Gamma 
mc.G <- rgamma(M, shape=a, rate=b)

## Generate a MC sample of size 1000 from Inverse Gamma 
mc.IG <- 1 / mc.G

## Generate a MC sample of size 1000 from Square root Inverse Gamma 
mc.SIG <- sqrt(1/mc.G)
```

```{r, fig.show="hold", out.width="50%"}
plot(1:M, mc.G, type="l", col=2, xlab="Iterations", ylab="MC sample", 
     main="Traceplot of the MC sample from Gamma")
hist(mc.G, breaks=50, freq=FALSE, xlab="X", main="Histogram of MC sample of X")
curve(dgamma(x, a, b), add=TRUE, col=2, lwd=2)

plot(1:M, mc.IG, type="l", col=3, xlab="Iterations", ylab="MC sample", 
     main="Traceplot of the MC sample from Inverse Gamma")
hist(mc.IG, breaks=50, freq=FALSE, xlab="Y", main="Histogram of MC sample of Y")
curve(dinvgamma(x, a, b), add=TRUE, col=3, lwd=2)

plot(1:M, mc.SIG, type="l", col=4, xlab="Iterations", ylab="MC sample", 
     main="Traceplot of the MC sample from Square root Inverse Gamma")
hist(mc.SIG, breaks=50, freq=FALSE, ylim=c(0, 2), xlab="Z", 
     main="Histogram of MC sample of Z")
curve(dsqrtinvgamma(x, a, b), add=TRUE, col=4, lwd=2)
```

```{r, fig.show="hold", out.width="33.3%", include=FALSE}
hist(mc.G, breaks=50, freq=FALSE)
curve(dgamma(x, a, b), add=TRUE, col=2, lwd=2)

hist(mc.IG, breaks=50, freq=FALSE)
curve(dinvgamma(x, a, b), add=TRUE, col=3, lwd=2)

hist(mc.SIG, breaks=50, freq=FALSE, ylim=c(0, 2))
curve(dsqrtinvgamma(x, a, b), add=TRUE, col=4, lwd=2)
```


```{r, include=FALSE}
hist(mc.G, breaks=50, freq=FALSE)
curve(dgamma(x, a, b), from=min(mc.G), to=max(mc.G), add=TRUE, col=2, lwd=2)

hist(mc.IG, breaks=50, freq=FALSE)
curve(dinvgamma(x, a, b), from=min(mc.IG), to=max(mc.IG), add=TRUE, col=3, lwd=2)

hist(mc.SIG, breaks=50, freq=FALSE, ylim=c(0, 2))
curve(dsqrtinvgamma(x, a, b), from=min(mc.SIG), to=max(mc.SIG), add=TRUE, col=4, lwd=2)
```

```{r}
## Gamma
meanG <- mean(mc.G)
medG <- median(mc.G)

## Inverse Gamma
meanIG <- mean(mc.IG)
medIG <- median(mc.IG)

## Square root Inverse Gamma
meanSIG <- mean(mc.SIG)
medSIG <- median(mc.SIG)
```

```{r}
df <- data.frame(
  c(meanG, meanIG, meanSIG), 
  c(medG, medIG, medSIG)
)
colnames(df) <- c("Sample Mean", "Sample Median")
rownames(df) <- c("G", "IG", "SIG")

knitr::kable(df, caption="Summary statistics", align="c")
```

We know that the random variables $X$, $Y$, and $Z$ have the following relation:
$$
\begin{aligned}
Y=\frac{1}{X} &\implies X=\frac{1}{Y} \\
Z=\sqrt{\frac{1}{X}} &\implies X=\frac{1}{Z^2}
\end{aligned}
$$
Let us see if such a relation holds for sample medians:
```{r}
cat(sprintf(
  "Median of X: %.4f\nOne over median of Y: %.4f\nOne over squared median of Z: %.4f", 
  medG, 1/medIG, 1/medSIG^2
))
```
We see that the transformation of random variables does not change the relation of medians for $X$, $Y$, and $Z$.

Let us further check if such a relation holds for sample means:
```{r}
cat(sprintf(
  "Mean of X: %.4f\nOne over mean of Y: %.4f\nOne over squared mean of Z: %.4f", 
  meanG, 1/meanIG, 1/meanSIG^2
))
```
We obtain different values for $X$, $Y$, and $Z$ if we do back-transformation operations on their means.

$$
\text{Median}(Y)=\text{Median}\left(\frac{1}{X} \right)=\frac{1}{\text{Median}(X)}
$$

$$
\text{Median}(Z)=\text{Median}\left(\sqrt{\frac{1}{X}} \right)=\sqrt{\frac{1}{\text{Median}(X)}}
$$

$$
\mathbb{E}[Y]=\mathbb{E}\left[\frac{1}{X} \right]\neq \frac{1}{\mathbb{E}[X]}
$$

$$
\mathbb{E}[Z]=\mathbb{E}\left[\sqrt{\frac{1}{X}} \right]\neq \sqrt{\frac{1}{\mathbb{E}[X]}}
$$

**Reasoning:** 

* The median value is fixed by its position.
* Inverse transformation only reverses the ranking (quantile) of $X$ but does not change the 50% quantile of $X$. Therefore, the inverse transformation can be operated directly on the median of $X$ in order to obtain the median of $Y$.
* Square root inverse transformation is simply to add square-root transformation on the top of inverse transformation. Square root transformation is a monotonic transformation and does not change the ranking (quantile) of the $Y$, which makes it possible for direct operations on the medians.
* This is not necessarily true for the mean values. Inverse transformation and square root transformation are nonlinear transformation while expectation only allows for linear operations (e.g. $\mathbb{E}[X+Y]=\mathbb{E}[X]+\mathbb{E}[Y]$, $\mathbb{E}[aX]=a\mathbb{E}[X]$). Since the mean and the median are different in the target Gamma distribution, we cannot tell anything about the relation between the sample means of $X$, $Y$, and $Z$.

<!-- Let us further look at the shape of the transformation and check if the Jensens inequality holds: -->

```{r,echo = F, fig.width=4, fig.height=3,fig.align='center', include=FALSE}
par(mfrow = c(1,2))
x <- seq(from =0, to = 2, length.out = 100)
plot(x=x, y = 1/x,type = "l")
plot(x=x, y = sqrt(1/x), type = "l")
```

```{r, include=FALSE}
#Because both are convex
1/meanG <= meanIG
sqrt(1/meanG) <= meanSIG
```



